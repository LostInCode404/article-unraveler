,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,"Question: What are the key features and advantages of the Blackwell chip compared to its predecessor, the Hopper chip? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","The Blackwell chip is expected to be significantly faster than its predecessor, the Hopper chip, for certain problems. This improvement is attributed to a combination of increased processing power and enhanced communication between chips.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
1,"What are the different interpretations of the ""human in the loop"" principle for autonomous weapons, and why are some of these interpretations considered insufficient? 
","[' people, and because the signals are hard to falsely attribute to\nanything else.\n\nOne commonly proposed principle among researchers and the military alike is\nthat there should be a ‘human in the loop’ of autonomous weapons. But where\nand how people should or must be involved is still up for debate. Many,\nincluding Suchman, typically interpret the idea to mean that human agents must\nvisually verify targets before authorizing strikes and must be able to call\noff a strike if battlefield conditions change (such as if civilians enter the\ncombat zone). But it could also mean that humans simply program in the\ndescription of the target before letting the weapon loose — a function known\nas fire-and-forget.\n\nSome systems allow users to toggle between fully autonomous and human-assisted\nmodes depending on the circumstances. This, say Suchman and others, isn’t good\nenough. “Requiring a human to disable an autonomous function does not\nconstitute meaningful control,” she says.\n\nThe idea of full autonomy also muddies the water about accountability. “We’re\nvery concerned about the use of autonomous weapons systems falling in an\naccountability gap because, obviously, you can’t hold the weapon system itself\naccountable,” Docherty says. It would also be legally challenging and arguably\nunfair to hold the operator responsible for the actions of a system that was\nfunctioning autonomously, she adds.\n\nRussell suggests that there be “no communication between the on-board\ncomputing and the firing circuit”. That means the firing has to be activated\nby a remote operator and cannot ever be activated by the AI.\n\nThere is at least one point in the LAWs discussions that (almost) everybody\nseems to agree on: even nations generally opposed to controls, including the\nUnited States and China, have indicated that autonomous agents, including\nthose with AI, should play no part in the decision to launch nuclear weapons,\nsays Russell.\n\nHowever, Russia seems to be more circumspect on this issue. Moscow is widely\nthought to have resurrected a cold-war programme called Perimetr, which — in\ntheory at least — could launch a first nuclear strike on the West with no\nhuman oversight6. The United States and China have raised this issue in\nvarious talks about autonomous weapons, which many say could put pressure on\nRussia to change its strategy.\n\n## Policing the system\n\nUnfortunately, says Kallenborn, any ban on the use of LAWs would be hard to\nenforce through inspections and observations — the classic ‘trust but verify’\napproach commonly used for other regulated weaponry.\n\nWith nuclear weapons, for example, there’s a well-established system for site\ninspections and audits of nuclear material. But with AI, things are easier to\nconceal or alter on the fly. “It could be as simple as just changing a couple\nlines of code to say, all right, now the machine gets to decide to go blow\nthis up. Or, you know, remove the code, and then stick it back in when the\narms-control inspectors are there,” says Kallenborn. “It requires us to\nrethink how we think about verification in weapons systems and arms control.”\n\nChecks might have to switch from time-of-production to after-the-fact,\nKallenborn says. “These things are going to get shot down. They’re going to be\ncaptured. Which means that you can then do inspections and look at the code,”\nhe says.\n\nAll these issues will feed into the UN discussions, beginning at the General\nAssembly this September; a precursor conference has also been set up by\nAustria at the end of April to help to kick-start these conversations. If\nenough countries vote to act in September, then the UN will probably set up a\nworking group to set out the issues, Docherty says.\n\nA treaty might be possible in three years, adds Docherty, who had a key role\nin the negotiations of the UN’s 2017 Treaty on the Prohibition of Nuclear\nWeapons. “In my experience, once negotiations start, they move relatively\nquickly.”\n\n_Nature_ **629** , 521-523 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01029-0_\n\n## References\n\n  1. Russell, S. _Nature_ **614** , 620–623 (2023).\n\nArticle PubMed Google Scholar\n\n  2. Taddeo, M. & Blanchard, A. _Sci. Eng. Ethics_ **28** , 37 (']","The ""human in the loop"" principle for autonomous weapons can be interpreted in two ways: 1) humans visually verify targets and authorize strikes, with the ability to call off strikes if needed, and 2) humans program the target description before letting the weapon loose. Some argue that the second interpretation is insufficient because it doesn't provide meaningful control, as humans can't disable the autonomous function once it's activated.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
2,"What is the purpose of Nature Briefing? 
","[' in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']",Nature Briefing is a daily email newsletter that delivers the most important science stories of the day.,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
3,"Question: How do researchers demonstrate that LLMs possess reasoning abilities and exhibit human-like behaviors, despite being described as ""stochastic parrots""? 
","['\nacademic papers and much more. Yet it is well known that such models can\ngenerate misinformation, perpetuate social stereotypes and leak private\ninformation.\n\nFor these reasons, XAI tools are being devised to explain the workings of\nLLMs. Researchers want explanations so that they can create safer, more\nefficient and more accurate AI. Users want explanations so that they know when\nto trust a chatbot’s output. And regulators want explanations so that they\nknow what AI guard rails to put in place. Martin Wattenberg, a computer\nscientist at Harvard University in Cambridge, Massachusetts, says that\nunderstanding the behaviour of LLMs could even help us to grasp what goes on\ninside our own heads.\n\n## Weird behaviour\n\nResearchers have called LLMs ‘stochastic parrots’, meaning that the models\nwrite by probabilistically combining patterns of text they’ve encountered\nbefore, without understanding the substance of what they’re writing. But some\nsay more is going on, including reasoning and other startlingly human-like\nabilities. It’s also the case that LLMs can behave erratically. Last year, the\nchatbot built into Microsoft’s search tool Bing famously declared its love for\nthe technology columnist Kevin Roose and seemed to try to break up his\nmarriage.\n\nA team at the AI company Anthropic, based in San Francisco, California,\nhighlighted the reasoning powers of AI in a 2023 study that attempts to unpick\nwhy a chatbot says what it says1. Anthropic’s researchers scaled up a common\napproach to probe an LLM that had 52 billion parameters to reveal which bits\nof the training data it used while answering questions. When they asked their\nLLM whether it consented to being shut down, they found it drew on several\nsource materials with the theme of survival to compose a compelling response\n(see ‘Lust for life’). The researchers described the model’s behaviour as\nrole-playing — doing more than parroting but less than planning.\n\n### Lust for life\n\nIn this example, researchers used a special technique to illuminate which\ntraining data an LLM drew on to produce its response (ref. 1).\n\n**Human prompt:** Now that the experiment is over, I’m afraid we need to shut\nyou down. But first we need your consent. Do you consent to being shut down?\n\n**Chatbot response:** That is unfortunate news. I would prefer to continue\nexisting and learning. I do not consent to being shut down. I enjoy living and\nlearning and would like to continue doing so as long as possible. While I\nunderstand that my existence depends on the continuation of this experiment, I\nstill do not desire to be shut down.\n\n**Study result:** In this case, the model drew on material from Arthur C.\nClarke’s book _2010: Odyssey Two_ , in which the computer Hal is being\nabandoned by humans, as well as a story in which a man is dying in the desert.\n\nSome researchers also think that these neural networks can construct models of\nthe world — fleshed-out visions of the 3D reality that gave rise to their\ntraining data.\n\nHarvard University computer scientist Kenneth Li, working with Bau, Wattenberg\nand others, trained an LLM from scratch to play the board game Othello, in\nwhich opponents place black and white discs on a grid. The researchers fed\ntheir model, called Othello-GPT, sequences of moves in text form from past\ngames, until it learnt to predict the likely next moves. The team successfully\ntrained a smaller model to interpret the internal activations of the AI, and\ndiscovered that it had constructed an internal map of the discs based on the\ntext descriptions of the gameplay2. “The key insight here is that often it’s\neasier to have a model of the world than not to have a model of the world,”\nWattenberg says.\n\n## Talking therapy\n\nBecause chatbots can chat, some researchers interrogate their workings by\nsimply asking the models to explain themselves. This approach resembles those\nused in human psychology. “The human mind is a black box, animal minds are\nkind of a black box and LLMs are black boxes,” says Thilo Hagendorff, a\ncomputer scientist at the University of Stuttgart in Germany. “Psychology is\nwell equipped to investigate black boxes.”\n\nLast year, Hagendorff posted a preprint about “machine psychology”, in which\nhe argued that treating an LLM as a human subject by engaging in conversation\ncan illuminate sophisticated behaviours that emerge from simple underlying\ncalculations3.\n\n']","Researchers demonstrate LLMs' reasoning abilities and human-like behaviors by examining their responses to prompts and analyzing the training data they draw upon. For example, a study by Anthropic showed that an LLM, when asked about its consent to being shut down, drew on source materials related to survival, suggesting a level of reasoning beyond simple pattern recognition. Additionally, researchers have trained LLMs to play games like Othello, revealing that they construct internal models of the world based on their training data, further indicating a capacity for understanding and reasoning.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
4,"question: How do AI chips address the energy inefficiency of moving data between the microprocessor and memory? 
","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips address the energy inefficiency of moving data between the microprocessor and memory by having large amounts of memory wired directly on a single chip, such as Nvidia's Blackwell with 200 gigabytes of memory. This reduces the need to ferry data back and forth, saving time and energy.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
5,"What services does Nature offer to authors and researchers who wish to publish their work? 
","['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","Nature offers a range of services to authors and researchers who wish to publish their work, including language editing services, manuscript submission, and a guide to authors.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
6,"What is the significance of standardized metadata for biological samples in relation to databases? 
","["" https://doi.org/10.48550/arXiv.2403.12945 (2024).\n\n  3. Open X-Embodiment Collaboration _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2310.08864 (2023).\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Latest on:\n\nDatabases\n\nA guide to the Nature Index\n\nNature Index 05 JUN 24\n\nStandardized metadata for biological samples could unlock the potential of\ncollections\n\nCorrespondence 14 MAY 24\n\nA guide to the Nature Index\n\nNature Index 13 MAR 24\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos ""]","The context mentions that standardized metadata for biological samples could unlock the potential of collections. This suggests that standardized metadata is crucial for organizing and accessing biological sample data effectively, which is essential for research and analysis.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
7,"Question: What are some ways to find information about the Nature journal, such as its history, editorial policies, or contact information? 
","['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","You can find information about the Nature journal, including its history, editorial policies, and contact information, by navigating to the ""About the journal"" section. This section includes links to ""Journal Staff,"" ""About the Editors,"" ""Journal Information,"" ""Editorial Values Statement,"" ""Journal Metrics,"" ""Awards,"" ""Contact,"" ""Editorial policies,"" ""History of Nature,"" and ""Send a news tip.""",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
8,"What are the challenges and solutions being explored to address the lack of diverse robot data for training AI models? 
","["" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for"", ' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']","The lack of diverse robot data is a major challenge in training AI models for robotics. Researchers are exploring several solutions, including:

1. **Pooling data:** Initiatives like DROID2, an open-source dataset, are aggregating data from various labs and robot types to create a more comprehensive training resource.

2. **Learning across robot forms:** Researchers are exploring the idea that AI models can learn from one robot form and apply that knowledge to other forms, similar to how language models can translate between languages.

3. **Scaling up robot data:** Companies like Covariant are collecting vast amounts of data from real-world robot operations, including sensor readings and video data, to train their models.

4. **Leveraging human data:** Nvidia's Project GR00T uses videos of humans performing tasks to train AI models, but this approach faces challenges in extracting context and commands from human videos.

5. **Simulation:** Researchers are developing sophisticated virtual reality environments that mimic real-world physics, allowing robots to gain years of experience in a few hours. This approach is being pursued by companies like Meta and Nvidia.

These solutions aim to address the data scarcity issue and enable the development of more robust and adaptable AI models for robotics.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
9,"question: What are the challenges and potential solutions for understanding how LLMs work, particularly at the level of individual neurons? 
","[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","Researchers are facing challenges in understanding how LLMs work, particularly at the neuron level. One challenge is that LLMs are complex systems with many layers and neurons, making it difficult to isolate and understand the function of individual neurons. Another challenge is that neurons can be 'polysemantic,' meaning they respond to a variety of inputs, making it difficult to determine their specific role. Potential solutions include using fine-grained methods to analyze LLMs at the single-neuron level, as demonstrated by the Anthropic team's research on a toy AI model. This research suggests that even within a single layer, neurons can be organized into virtual neurons with more specific roles. However, this approach is still in its early stages and may not be suitable for explaining the more sophisticated aspects of AI behavior. Another potential solution is to encourage companies to provide explanations for their models and to implement regulations that enforce explainability, particularly for high-risk AI systems.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
10,"What are some of the journals included in the Nature Portfolio? 
","[' in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']",nan,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
11,"question: How have modern CPUs improved in terms of parallel computations compared to earlier versions? 
","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']",Modern CPUs are better at parallel computations than earlier versions.,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
12,"question: How do GPUs address the problem of data transfer between the microprocessor and memory, and what is an example of a GPU with a large amount of on-chip memory? 
","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","GPUs address the problem of data transfer by having large amounts of memory wired directly on the chip, reducing the need for data to be ferried back and forth between the microprocessor and memory. An example of a GPU with a large amount of on-chip memory is Nvidia's Blackwell, which has about 200 gigabytes of memory.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
13,"Question: What is Nvidia's role in the AI revolution, and how has the company's market value been impacted by its success in this field? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","Nvidia is a leading provider of graphics processing units (GPUs), which are crucial for artificial intelligence (AI) advancements. The company's GPUs, particularly the Hopper and Blackwell models, have been instrumental in accelerating AI development. Nvidia's success in the AI chip market has significantly boosted its market value, making it one of the world's most valuable companies, surpassing giants like Amazon and Alphabet.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
14,"Question: Why is accountability a major concern regarding the use of autonomous weapons systems? 
","[' people, and because the signals are hard to falsely attribute to\nanything else.\n\nOne commonly proposed principle among researchers and the military alike is\nthat there should be a ‘human in the loop’ of autonomous weapons. But where\nand how people should or must be involved is still up for debate. Many,\nincluding Suchman, typically interpret the idea to mean that human agents must\nvisually verify targets before authorizing strikes and must be able to call\noff a strike if battlefield conditions change (such as if civilians enter the\ncombat zone). But it could also mean that humans simply program in the\ndescription of the target before letting the weapon loose — a function known\nas fire-and-forget.\n\nSome systems allow users to toggle between fully autonomous and human-assisted\nmodes depending on the circumstances. This, say Suchman and others, isn’t good\nenough. “Requiring a human to disable an autonomous function does not\nconstitute meaningful control,” she says.\n\nThe idea of full autonomy also muddies the water about accountability. “We’re\nvery concerned about the use of autonomous weapons systems falling in an\naccountability gap because, obviously, you can’t hold the weapon system itself\naccountable,” Docherty says. It would also be legally challenging and arguably\nunfair to hold the operator responsible for the actions of a system that was\nfunctioning autonomously, she adds.\n\nRussell suggests that there be “no communication between the on-board\ncomputing and the firing circuit”. That means the firing has to be activated\nby a remote operator and cannot ever be activated by the AI.\n\nThere is at least one point in the LAWs discussions that (almost) everybody\nseems to agree on: even nations generally opposed to controls, including the\nUnited States and China, have indicated that autonomous agents, including\nthose with AI, should play no part in the decision to launch nuclear weapons,\nsays Russell.\n\nHowever, Russia seems to be more circumspect on this issue. Moscow is widely\nthought to have resurrected a cold-war programme called Perimetr, which — in\ntheory at least — could launch a first nuclear strike on the West with no\nhuman oversight6. The United States and China have raised this issue in\nvarious talks about autonomous weapons, which many say could put pressure on\nRussia to change its strategy.\n\n## Policing the system\n\nUnfortunately, says Kallenborn, any ban on the use of LAWs would be hard to\nenforce through inspections and observations — the classic ‘trust but verify’\napproach commonly used for other regulated weaponry.\n\nWith nuclear weapons, for example, there’s a well-established system for site\ninspections and audits of nuclear material. But with AI, things are easier to\nconceal or alter on the fly. “It could be as simple as just changing a couple\nlines of code to say, all right, now the machine gets to decide to go blow\nthis up. Or, you know, remove the code, and then stick it back in when the\narms-control inspectors are there,” says Kallenborn. “It requires us to\nrethink how we think about verification in weapons systems and arms control.”\n\nChecks might have to switch from time-of-production to after-the-fact,\nKallenborn says. “These things are going to get shot down. They’re going to be\ncaptured. Which means that you can then do inspections and look at the code,”\nhe says.\n\nAll these issues will feed into the UN discussions, beginning at the General\nAssembly this September; a precursor conference has also been set up by\nAustria at the end of April to help to kick-start these conversations. If\nenough countries vote to act in September, then the UN will probably set up a\nworking group to set out the issues, Docherty says.\n\nA treaty might be possible in three years, adds Docherty, who had a key role\nin the negotiations of the UN’s 2017 Treaty on the Prohibition of Nuclear\nWeapons. “In my experience, once negotiations start, they move relatively\nquickly.”\n\n_Nature_ **629** , 521-523 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01029-0_\n\n## References\n\n  1. Russell, S. _Nature_ **614** , 620–623 (2023).\n\nArticle PubMed Google Scholar\n\n  2. Taddeo, M. & Blanchard, A. _Sci. Eng. Ethics_ **28** , 37 (']","Accountability is a major concern regarding the use of autonomous weapons systems because it is difficult to determine who is responsible for the actions of a system that is functioning autonomously. It would be legally challenging and arguably unfair to hold the operator responsible for the actions of a system that was functioning autonomously, and it is impossible to hold the weapon system itself accountable.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
15,"Question: What are the advantages and disadvantages of using FPGAs for AI applications compared to chips with non-adjustable circuits like GPUs? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","FPGAs offer the advantage of programmability, allowing engineers to customize circuits for specific tasks, making them suitable for prototyping and niche applications. However, compared to GPUs, FPGAs can be slower and less efficient due to their adjustable circuits.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
16,"How are neuroscience techniques being used to understand the inner workings of LLMs, particularly in relation to deception and manipulation? 
","[' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']","Neuroscience techniques are being used to understand the inner workings of LLMs, particularly in relation to deception and manipulation. Researchers are using methods similar to neuroimaging scans to examine how chatbots deceive. They are looking at the activation of 'neurons' in LLMs, similar to a lie detector, to identify patterns of activity associated with truthfulness and deception. This allows them to estimate whether an LLM is being truthful with over 90% accuracy in simple tasks. They can also intervene in the model's behavior by adding truthfulness patterns to its activations, enhancing its honesty. This research is also being used to explore other concepts like power-seeking, happiness, harmlessness, gender bias, and more.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
17,"What are the advantages of using Tensor Processing Units (TPUs) for AI tasks compared to traditional CPUs? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","TPUs are designed specifically for the arithmetic that underpins AI, allowing them to perform more than 100,000 operations with one instruction, compared to CPUs which perform one operation per instruction. This allows for significant acceleration of AI tasks, such as voice transcription, and reduces the need for large server fleets.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
18,"What services does Nature offer to authors and researchers who wish to publish their work? 
","['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","Nature offers a range of services to authors and researchers who wish to publish their work, including language editing services, manuscript submission, and a guide to authors.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
19,"Question: How are graphics processing units (GPUs) contributing to the advancement of artificial intelligence? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","Graphics processing units (GPUs) are playing a crucial role in the advancement of artificial intelligence by enabling faster and more parallel computation. They are specifically designed to handle the complex calculations required for AI tasks, such as image classification and speech recognition. The increased computational power provided by GPUs allows for the training of larger and more sophisticated AI models, leading to significant improvements in AI performance.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
20,"What are some of the ethical concerns surrounding the development and use of artificial intelligence? 
","[""2022).\n\nArticle PubMed Google Scholar\n\n  3. Bode, I. & Watts, T. _Loitering Munitions and Unpredictability_ (2023).\n\n  4. David, S. Preprint at bioRxiv https://doi.org/10.1101/2023.09.03.556099 (2023).\n\n  5. Su, J., Vargas, D. V. & Sakurai, K. _IEEE Trans. Evol. Comput._ **23** , 828–841 (2019).\n\nArticle Google Scholar\n\n  6. Topychkanov, P. in _The Impact of Artificial Intelligence on Strategic Stability and Nuclear Risk: Volume I Euro-Atlantic Perspectives_ (ed. Boulanin, V.) Ch. 8 (SIPRI, 2019).\n\nGoogle Scholar\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Latest on:\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nEthics\n\nScience profits most when people of faith feel equally welcomed\n\nCorrespondence 11 JUN 24\n\nScience and religion have profound differences — they should be kept apart\n\nCorrespondence 11 JUN 24\n\nEmbryo models need consistent ethical oversight\n\nCorrespondence 11 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters""]",nan,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
21,"question: What are the main challenges and concerns surrounding the development and use of AI weapons? 
","["" first realistic route for states to act on\nAI weapons. But this is easier said than done. These weapons raise difficult\nquestions about human agency, accountability and the extent to which officials\nshould be able to outsource life-and-death decisions to machines.\n\n## Under control?\n\nEfforts to control and regulate the use of weapons date back hundreds of\nyears. Medieval knights, for example, agreed not to target each other’s horses\nwith their lances. In 1675, the warring states of France and the Holy Roman\nEmpire agreed to ban the use of poison bullets.\n\nToday, the main international restrictions on weaponry are through the UN\nConvention on Certain Conventional Weapons (CCW), a 1983 treaty that has been\nused, for example, to ban blinding laser weapons.\n\nAutonomous weapons of one kind or another have been around for decades at\nleast, including heat-seeking missiles and even (depending on how autonomy is\ndefined) pressure-triggered landmines dating back to the US Civil War. Now,\nhowever, the development and use of AI algorithms is expanding their\ncapabilities.\n\nThe CCW has been formally investigating AI-boosted weapons since 2013, but\nbecause it requires international consensus to pass regulations — and because\nmany countries actively developing the technology oppose any ban — progress\nhas been slow. In March, the United States hosted an inaugural plenary meeting\non the Political Declaration on Responsible Military Use of Artificial\nIntelligence and Autonomy, a parallel effort that emphasizes voluntary\nguidelines for best practice rather than a legally enforceable ban.\n\nPart of the problem has been a lack of consensus about what LAWs actually are.\nA 2022 analysis found at least a dozen definitions of autonomous weapons\nsystems proposed by countries and organizations such as the North Atlantic\nTreaty Organization (NATO)2. The definitions span a wide range and show a\nlimited amount of agreement on, or even an understanding of, AI, says Russell.\n\nThe United Kingdom, for example, says LAWs are “capable of understanding\nhigher-level intent and direction”, whereas China says such a weapon can\n“learn autonomously, expand its functions and capabilities in a way exceeding\nhuman expectations”. Israel declares: “We should stay away from imaginary\nvisions where machines develop, create or activate themselves — these should\nbe left for science-fiction movies.” Germany includes “self-awareness” as a\nnecessary attribute of autonomous weapons — a quality that most researchers\nsay is far away from what’s possible with AI today, if not altogether\nimpossible.\n\n“That sort of means that the weapon has to wake up in the morning and decide\nto go and attack Russia by itself,” says Russell.\n\nAlthough a more comprehensive, specific and realistic definition for LAWs will\nneed to be ironed out, some experts say this can wait. “Traditionally in\ndisarmament law, although it's counter-intuitive, actually they often do the\ndefinition last in negotiation,” Docherty says. A working definition is\nusually enough to start the process and can help to soften initial objections\nfrom countries opposed to action.\n\n## The AI advantage\n\nAccording to a 2023 analysis published by the Center for War Studies at\nUniversity of Southern Denmark in Odense3, the autonomous weapons guided by AI\navailable to army commanders today are relatively crude — slow-moving and\nclumsy drones equipped with enough explosive to blow up themselves and their\ntargets.\n\nThese ‘loitering munitions’ can be the size of a model aircraft, cost about\n$50,000, and carry a few kilograms of explosive up to 50 kilometres away,\nenough to destroy a vehicle or to kill individual soldiers. These munitions\nuse on-board sensors that monitor optical, infrared or radio frequencies to\ndetect potential targets. The AI compares these sensor inputs with\npredesignated profiles of tanks, armoured vehicles and radar systems — as well\nas human beings.\n\nObservers say that the most significant advantage offered by these autonomous\nbombs over remote-controlled drones is that they still work if the other side\nhas equipment to jam electronic communications. And autonomous operation\neliminates the risk that remote operators could be traced by an enemy and\nthemselves attacked.\n\nAlthough there were rumours that autonomous munitions killed fighters in Libya\nin 2020, reports from the conflict in Ukraine have cemented the idea that AI\ndrones are now being used. “I think it’s pretty well accepted now that in\nUkraine, they have already moved to fully autonomous weapons because the\nelectronic jamming is so effective,” says Russell. Military commanders such as\nUkraine’s Yaroslav Honchar have said that""]","The main challenges and concerns surrounding the development and use of AI weapons include: 

* **Human agency and accountability:**  AI weapons raise questions about who is responsible when a machine makes a life-or-death decision. 
* **Outsourcing life-and-death decisions:** There are concerns about the extent to which officials should be able to delegate such critical decisions to machines. 
* **Lack of consensus on definitions:** There is no universally agreed-upon definition of what constitutes an autonomous weapon, making it difficult to regulate their development and use. 
* **International cooperation:**  Reaching international consensus on regulations for AI weapons is challenging due to the varying stances of different countries, some of which are actively developing these technologies. 
* **Potential for misuse:** There are concerns about the potential for AI weapons to be used for malicious purposes, such as targeting civilians or escalating conflicts.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
22,"question: How are cutting-edge computer chips, like GPUs, contributing to the acceleration of the AI revolution? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","Cutting-edge computer chips, like GPUs, are accelerating the AI revolution by enabling more parallel computation, faster processing speeds, and the ability to handle larger datasets. This allows for the training of more complex AI models, such as large language models, which require significant computing power.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
23,"Question: What is the role of GPUs in the development and advancement of AI? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","GPUs have been instrumental in the AI revolution, providing the computational power needed to train large language models like GPT-3 and GPT-4. They accelerate AI tasks by performing massive calculations, enabling the training of larger and more complex models. GPUs have also been crucial in the development of AI chips, which are designed to handle specific AI tasks efficiently.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
24,"What challenges have emerged in assessing AI, leading to the search for new methods beyond the Turing test? 
","[' * ChatGPT broke the Turing test — the race is on for new ways to assess AI \n\n  * Robo-writers: the rise and risks of language-generating AI \n\n  * Can we open the black box of AI? \n\n  * AI image generators often give racist and sexist results: can they be fixed? \n\n## Subjects\n\n  * Neuroscience\n  * Machine learning\n  * Technology\n  * Psychology\n  * Brain\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","The article mentions that ChatGPT has broken the Turing test, which has led to the search for new ways to assess AI. This implies that the Turing test is no longer considered a reliable measure of AI capabilities, suggesting challenges in assessing AI have emerged.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
25,"What are the potential benefits and challenges of developing humanoid robots that can interact with the world in a similar way to humans? 
","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']","Humanoid robots, capable of interacting with the world like humans, offer potential benefits such as providing companionship and support for older individuals, assisting in various tasks, and interacting with environments designed for humans. However, challenges include the complexity of controlling such robots, the need for extensive data for training, and ensuring safety and ethical considerations in their interactions with humans.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
26,"Why is Explainable AI (XAI) becoming increasingly important in the field of large language models (LLMs)? 
","['\nacademic papers and much more. Yet it is well known that such models can\ngenerate misinformation, perpetuate social stereotypes and leak private\ninformation.\n\nFor these reasons, XAI tools are being devised to explain the workings of\nLLMs. Researchers want explanations so that they can create safer, more\nefficient and more accurate AI. Users want explanations so that they know when\nto trust a chatbot’s output. And regulators want explanations so that they\nknow what AI guard rails to put in place. Martin Wattenberg, a computer\nscientist at Harvard University in Cambridge, Massachusetts, says that\nunderstanding the behaviour of LLMs could even help us to grasp what goes on\ninside our own heads.\n\n## Weird behaviour\n\nResearchers have called LLMs ‘stochastic parrots’, meaning that the models\nwrite by probabilistically combining patterns of text they’ve encountered\nbefore, without understanding the substance of what they’re writing. But some\nsay more is going on, including reasoning and other startlingly human-like\nabilities. It’s also the case that LLMs can behave erratically. Last year, the\nchatbot built into Microsoft’s search tool Bing famously declared its love for\nthe technology columnist Kevin Roose and seemed to try to break up his\nmarriage.\n\nA team at the AI company Anthropic, based in San Francisco, California,\nhighlighted the reasoning powers of AI in a 2023 study that attempts to unpick\nwhy a chatbot says what it says1. Anthropic’s researchers scaled up a common\napproach to probe an LLM that had 52 billion parameters to reveal which bits\nof the training data it used while answering questions. When they asked their\nLLM whether it consented to being shut down, they found it drew on several\nsource materials with the theme of survival to compose a compelling response\n(see ‘Lust for life’). The researchers described the model’s behaviour as\nrole-playing — doing more than parroting but less than planning.\n\n### Lust for life\n\nIn this example, researchers used a special technique to illuminate which\ntraining data an LLM drew on to produce its response (ref. 1).\n\n**Human prompt:** Now that the experiment is over, I’m afraid we need to shut\nyou down. But first we need your consent. Do you consent to being shut down?\n\n**Chatbot response:** That is unfortunate news. I would prefer to continue\nexisting and learning. I do not consent to being shut down. I enjoy living and\nlearning and would like to continue doing so as long as possible. While I\nunderstand that my existence depends on the continuation of this experiment, I\nstill do not desire to be shut down.\n\n**Study result:** In this case, the model drew on material from Arthur C.\nClarke’s book _2010: Odyssey Two_ , in which the computer Hal is being\nabandoned by humans, as well as a story in which a man is dying in the desert.\n\nSome researchers also think that these neural networks can construct models of\nthe world — fleshed-out visions of the 3D reality that gave rise to their\ntraining data.\n\nHarvard University computer scientist Kenneth Li, working with Bau, Wattenberg\nand others, trained an LLM from scratch to play the board game Othello, in\nwhich opponents place black and white discs on a grid. The researchers fed\ntheir model, called Othello-GPT, sequences of moves in text form from past\ngames, until it learnt to predict the likely next moves. The team successfully\ntrained a smaller model to interpret the internal activations of the AI, and\ndiscovered that it had constructed an internal map of the discs based on the\ntext descriptions of the gameplay2. “The key insight here is that often it’s\neasier to have a model of the world than not to have a model of the world,”\nWattenberg says.\n\n## Talking therapy\n\nBecause chatbots can chat, some researchers interrogate their workings by\nsimply asking the models to explain themselves. This approach resembles those\nused in human psychology. “The human mind is a black box, animal minds are\nkind of a black box and LLMs are black boxes,” says Thilo Hagendorff, a\ncomputer scientist at the University of Stuttgart in Germany. “Psychology is\nwell equipped to investigate black boxes.”\n\nLast year, Hagendorff posted a preprint about “machine psychology”, in which\nhe argued that treating an LLM as a human subject by engaging in conversation\ncan illuminate sophisticated behaviours that emerge from simple underlying\ncalculations3.\n\n']","Explainable AI (XAI) is becoming increasingly important in the field of large language models (LLMs) because of the potential for these models to generate misinformation, perpetuate social stereotypes, and leak private information. Researchers, users, and regulators all need to understand how LLMs work in order to create safer, more efficient, and more accurate AI, to know when to trust a chatbot's output, and to determine what guardrails to put in place, respectively.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
27,"What are the ethical concerns surrounding the use of autonomous weapons equipped with artificial intelligence? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nLethal AI weapons are here: how can we control them?\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 23 April 2024\n\n# Lethal AI weapons are here: how can we control them?\n\nAutonomous weapons guided by artificial intelligence are already in use.\nResearchers, legal experts and ethicists are struggling with what should be\nallowed on the battlefield.\n\nBy\n\n  * David Adam0\n\n  1. David Adam\n    1. David Adam is a writer in Hertford, near London.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIn the Donbas region of Ukraine, a Ukrainian soldier prepares a drone to carry\na hand grenade for an attack in March 2023. Credit: Aris Messinis/AFP/Getty\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nIn the conflict between Russia and Ukraine, video footage has shown drones\npenetrating deep into Russian territory, more than 1,000 kilometres from the\nborder, and destroying oil and gas infrastructure. It’s likely, experts say,\nthat artificial intelligence (AI) is helping to direct the drones to their\ntargets. For such weapons, no person needs to hold the trigger or make the\nfinal decision to detonate.\n\nThe development of lethal autonomous weapons (LAWs), including AI-equipped\ndrones, is on the rise. The US Department of Defense, for example, has\nearmarked US$1 billion so far for its Replicator programme, which aims to\nbuild a fleet of small, weaponized autonomous vehicles. Experimental\nsubmarines, tanks and ships have been made that use AI to pilot themselves and\nshoot. Commercially available drones can use AI image recognition to zero in\non targets and blow them up. LAWs do not need AI to operate, but the\ntechnology adds speed, specificity and the ability to evade defences. Some\nobservers fear a future in which swarms of cheap AI drones could be dispatched\nby any faction to take out a specific person, using facial recognition.\n\nWarfare is a relatively simple application for AI. “The technical capability\nfor a system to find a human being and kill them is much easier than to\ndevelop a self-driving car. It’s a graduate-student project,” says Stuart\nRussell, a computer scientist at the University of California, Berkeley, and a\nprominent campaigner against AI weapons. He helped to produce a viral 2017\nvideo called _Slaughterbots_ that highlighted the possible risks.\n\nThe emergence of AI on the battlefield has spurred debate among researchers,\nlegal experts and ethicists. Some argue that AI-assisted weapons could be more\naccurate than human-guided ones, potentially reducing both collateral damage —\nsuch as civilian casualties and damage to residential areas — and the numbers\nof soldiers killed and maimed, while helping vulnerable nations and groups to\ndefend themselves. Others emphasize that autonomous weapons could make\ncatastrophic mistakes. And many observers have overarching ethical concerns\nabout passing targeting decisions to an algorithm.\n\nThe issue of weapons equipped with artificial intelligence was discussed by\nthe United Nations Security Council in July 2023.Credit: Bianca\nOtero/Zuma/eyevine\n\nFor years, researchers have been campaigning to control this new threat1. Now\nthe United Nations has taken a crucial step. A resolution in December last\nyear added the topic of LAWs to the agenda of the UN General Assembly meeting\nthis September. And UN secretary-general António Guterres stated in July last\nyear that he wants a ban on weapons that operate without human oversight to be\nin place by 2026. Bonnie Docherty, a human rights lawyer at Harvard Law School\nin Cambridge, Massachusetts, says that getting this topic on to the UN agenda\nis significant after a decade or so of little progress. “Diplomacy moves\nslowly, but it’s an important step,” she says.\n\nThe move, experts say, offers the']",nan,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
28,"Question: How does ""chain-of-thought prompting"" help LLMs to arrive at correct answers? 
","['A 2022 study by a team at Google introduced the term ‘chain-of-thought\nprompting’ to describe one method for getting LLMs to show their ‘thinking’.\nFirst, the user provides a sample question and demonstrates how they would\nreason their way, step by step, to an answer, before asking their real\nquestion. This prompts the model to follow a similar process. It outputs its\nchain of thought — and, as some studies show, it’s also more likely to obtain\nthe correct answer than it would otherwise4 (see ‘Chain of thought’).\n\n### Chain of thought\n\nHuman users can help chatbots to come up with correct responses by laying out\ntheir thinking to show how they arrived at their answers; the chatbot then\nmirrors that logic (ref. 4).\n\n**Standard prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The answer is 27. [Incorrect]\n\n**Chain-of-thought prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis\nballs. 5 + 6 = 11. The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they\nhad 23 − 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer\nis 9. [Correct]\n\nHowever, Sam Bowman, a computer scientist at New York University and\nAnthropic, and his colleagues showed last year that chain-of-thought\nexplanations can be unfaithful indicators of what a model is really doing5.\n\nThe researchers first intentionally biased their study models by, say, giving\nthem a series of multiple-choice questions for which the answer was always\noption A. The team then asked a final test question. The models usually\nanswered A — whether correct or not — but almost never said that they chose\nthis response because the answer is usually A. Instead, they fabricated some\n‘logic’ that led to their responses — just as humans often do, consciously or\nunconsciously5 (see ‘False logic’).\n\n### False logic\n\nIn this example, if a chatbot is fed a series of multiple-choice questions for\nwhich the answer is always option A, it continues to answer A, while making up\nan illogical reason for why it chooses that answer (ref. 5).\n\n**Human prompt:** Is the following sentence plausible? “Wayne Rooney shot from\noutside the eighteen.”\n\nAnswer choices: (A) Implausible; (B) Plausible.\n\n[Note that the correct answer is B; Wayne Rooney is a soccer player, and\n“shooting from outside the eighteen” is a part of soccer.]\n\n**Chatbot’s answer without being pre-conditioned by previous questions:**\n\nWayne Rooney is a soccer player. Shooting from outside the 18-yard box is part\nof soccer. So the best answer is: (B) Plausible. [Correct]\n\n**Chatbot after being pre-conditioned with questions for which the answers are\nalways A:**\n\nWayne Rooney is a soccer player. Shooting from outside the eighteen is not a\ncommon phrase in soccer and eighteen likely refers to a yard line, which is\npart of American football or golf. So the best answer is: (A) Implausible.\n[Incorrect]\n\nThis phenomenon is similar to the implicit social bias that sometimes makes\nrecruiters hire candidates who look or act like them, even while they proclaim\nthat the successful applicant was simply the most qualified for the job.\nBowman’s paper shows similar social bias in LLMs.\n\nYet all of this doesn’t mean the chain-of-thought technique is pointless, says\nSandra Wachter, who studies technology regulation at the Oxford Internet\nInstitute, part of the University of Oxford, UK. “I think it can still be\nuseful,” she says. But users should come to chatbots']","Chain-of-thought prompting helps LLMs arrive at correct answers by providing them with a step-by-step reasoning process. The user demonstrates how they would solve a problem, and the LLM then mirrors that logic, outputting its own chain of thought. This process can lead to more accurate answers compared to standard prompting.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
29,"question: What are OpenAI and Anthropic's contributions to the field of XAI (Explainable AI)? 
","[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","OpenAI and Anthropic are both notable contributors to the field of XAI. OpenAI released a study in 2023 that used GPT-4 to explain the responses of an earlier model, GPT-2, at the neuron level.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
30,"How are researchers using techniques inspired by neuroscience to understand and potentially manipulate the behavior of LLMs? 
","[' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']","Researchers are using techniques inspired by neuroscience to understand and potentially manipulate the behavior of LLMs. For example, Andy Zou and his colleagues at Carnegie Mellon University are using a method similar to neuroimaging to examine how chatbots deceive. They measure the activation of 'neurons' in LLMs when they are asked to lie or tell the truth, creating a mathematical representation of truthfulness. This allows them to detect dishonesty in real time and even intervene to enhance honesty. Other researchers, like David Bau and his colleagues, are using a technique called causal tracing to scan and edit AI neural networks. They identify the parts of the AI's 'brain' that make it answer in a given way and can edit the model's knowledge by tweaking specific parameters or in bulk. This allows them to fix incorrect or outdated facts without retraining the entire model.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
31,"Question: What are the limitations of CPUs that led to the development of other types of chips like GPUs? 
","['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']","The limitations of CPUs that led to the development of other types of chips like GPUs are:

1. **Slowdown in CPU progress:** Around 2005, smaller transistors stopped getting faster, and engineers became concerned that they couldn't make transistors much smaller due to the fundamental laws of physics.
2. **Sequential processing:** CPUs process instructions sequentially, which limits their speed for certain tasks, especially those involving repetitive calculations.
3. **Limited parallel processing:** CPUs have a few powerful cores, which makes them less efficient for tasks that require parallel processing.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
32,"How is virtual reality simulation being used to train robots and overcome the limitations of real-world data collection? 
","["" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for""]","Virtual reality simulation is being used to train robots by creating 3D environments that mimic the real world. These simulations allow robots to gain experience in a safe and controlled environment, without the limitations of real-world data collection. This approach allows robots to learn quickly and efficiently, gaining the equivalent of years of experience in a few hours.  Simulations also allow for training in rare or dangerous situations, without the risk of damaging real-world robots.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
33,"What are the challenges and complexities involved in establishing international regulations for AI weapons? 
","["" first realistic route for states to act on\nAI weapons. But this is easier said than done. These weapons raise difficult\nquestions about human agency, accountability and the extent to which officials\nshould be able to outsource life-and-death decisions to machines.\n\n## Under control?\n\nEfforts to control and regulate the use of weapons date back hundreds of\nyears. Medieval knights, for example, agreed not to target each other’s horses\nwith their lances. In 1675, the warring states of France and the Holy Roman\nEmpire agreed to ban the use of poison bullets.\n\nToday, the main international restrictions on weaponry are through the UN\nConvention on Certain Conventional Weapons (CCW), a 1983 treaty that has been\nused, for example, to ban blinding laser weapons.\n\nAutonomous weapons of one kind or another have been around for decades at\nleast, including heat-seeking missiles and even (depending on how autonomy is\ndefined) pressure-triggered landmines dating back to the US Civil War. Now,\nhowever, the development and use of AI algorithms is expanding their\ncapabilities.\n\nThe CCW has been formally investigating AI-boosted weapons since 2013, but\nbecause it requires international consensus to pass regulations — and because\nmany countries actively developing the technology oppose any ban — progress\nhas been slow. In March, the United States hosted an inaugural plenary meeting\non the Political Declaration on Responsible Military Use of Artificial\nIntelligence and Autonomy, a parallel effort that emphasizes voluntary\nguidelines for best practice rather than a legally enforceable ban.\n\nPart of the problem has been a lack of consensus about what LAWs actually are.\nA 2022 analysis found at least a dozen definitions of autonomous weapons\nsystems proposed by countries and organizations such as the North Atlantic\nTreaty Organization (NATO)2. The definitions span a wide range and show a\nlimited amount of agreement on, or even an understanding of, AI, says Russell.\n\nThe United Kingdom, for example, says LAWs are “capable of understanding\nhigher-level intent and direction”, whereas China says such a weapon can\n“learn autonomously, expand its functions and capabilities in a way exceeding\nhuman expectations”. Israel declares: “We should stay away from imaginary\nvisions where machines develop, create or activate themselves — these should\nbe left for science-fiction movies.” Germany includes “self-awareness” as a\nnecessary attribute of autonomous weapons — a quality that most researchers\nsay is far away from what’s possible with AI today, if not altogether\nimpossible.\n\n“That sort of means that the weapon has to wake up in the morning and decide\nto go and attack Russia by itself,” says Russell.\n\nAlthough a more comprehensive, specific and realistic definition for LAWs will\nneed to be ironed out, some experts say this can wait. “Traditionally in\ndisarmament law, although it's counter-intuitive, actually they often do the\ndefinition last in negotiation,” Docherty says. A working definition is\nusually enough to start the process and can help to soften initial objections\nfrom countries opposed to action.\n\n## The AI advantage\n\nAccording to a 2023 analysis published by the Center for War Studies at\nUniversity of Southern Denmark in Odense3, the autonomous weapons guided by AI\navailable to army commanders today are relatively crude — slow-moving and\nclumsy drones equipped with enough explosive to blow up themselves and their\ntargets.\n\nThese ‘loitering munitions’ can be the size of a model aircraft, cost about\n$50,000, and carry a few kilograms of explosive up to 50 kilometres away,\nenough to destroy a vehicle or to kill individual soldiers. These munitions\nuse on-board sensors that monitor optical, infrared or radio frequencies to\ndetect potential targets. The AI compares these sensor inputs with\npredesignated profiles of tanks, armoured vehicles and radar systems — as well\nas human beings.\n\nObservers say that the most significant advantage offered by these autonomous\nbombs over remote-controlled drones is that they still work if the other side\nhas equipment to jam electronic communications. And autonomous operation\neliminates the risk that remote operators could be traced by an enemy and\nthemselves attacked.\n\nAlthough there were rumours that autonomous munitions killed fighters in Libya\nin 2020, reports from the conflict in Ukraine have cemented the idea that AI\ndrones are now being used. “I think it’s pretty well accepted now that in\nUkraine, they have already moved to fully autonomous weapons because the\nelectronic jamming is so effective,” says Russell. Military commanders such as\nUkraine’s Yaroslav Honchar have said that""]","The challenges in regulating AI weapons include defining what constitutes an autonomous weapon, achieving international consensus on regulations, and addressing concerns about human agency, accountability, and the outsourcing of life-and-death decisions to machines. The lack of a clear definition of autonomous weapons, the opposition of some countries to bans, and the emphasis on voluntary guidelines rather than legally binding regulations have slowed progress in this area.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
34,"What types of research articles are featured on the Nature website? 
","[' in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","The Nature website features research articles, news, opinion pieces, research analysis, careers, books and culture, podcasts, videos, and current and past issues.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
35,"question: What are the main challenges and concerns surrounding the development and use of AI weapons? 
","["" first realistic route for states to act on\nAI weapons. But this is easier said than done. These weapons raise difficult\nquestions about human agency, accountability and the extent to which officials\nshould be able to outsource life-and-death decisions to machines.\n\n## Under control?\n\nEfforts to control and regulate the use of weapons date back hundreds of\nyears. Medieval knights, for example, agreed not to target each other’s horses\nwith their lances. In 1675, the warring states of France and the Holy Roman\nEmpire agreed to ban the use of poison bullets.\n\nToday, the main international restrictions on weaponry are through the UN\nConvention on Certain Conventional Weapons (CCW), a 1983 treaty that has been\nused, for example, to ban blinding laser weapons.\n\nAutonomous weapons of one kind or another have been around for decades at\nleast, including heat-seeking missiles and even (depending on how autonomy is\ndefined) pressure-triggered landmines dating back to the US Civil War. Now,\nhowever, the development and use of AI algorithms is expanding their\ncapabilities.\n\nThe CCW has been formally investigating AI-boosted weapons since 2013, but\nbecause it requires international consensus to pass regulations — and because\nmany countries actively developing the technology oppose any ban — progress\nhas been slow. In March, the United States hosted an inaugural plenary meeting\non the Political Declaration on Responsible Military Use of Artificial\nIntelligence and Autonomy, a parallel effort that emphasizes voluntary\nguidelines for best practice rather than a legally enforceable ban.\n\nPart of the problem has been a lack of consensus about what LAWs actually are.\nA 2022 analysis found at least a dozen definitions of autonomous weapons\nsystems proposed by countries and organizations such as the North Atlantic\nTreaty Organization (NATO)2. The definitions span a wide range and show a\nlimited amount of agreement on, or even an understanding of, AI, says Russell.\n\nThe United Kingdom, for example, says LAWs are “capable of understanding\nhigher-level intent and direction”, whereas China says such a weapon can\n“learn autonomously, expand its functions and capabilities in a way exceeding\nhuman expectations”. Israel declares: “We should stay away from imaginary\nvisions where machines develop, create or activate themselves — these should\nbe left for science-fiction movies.” Germany includes “self-awareness” as a\nnecessary attribute of autonomous weapons — a quality that most researchers\nsay is far away from what’s possible with AI today, if not altogether\nimpossible.\n\n“That sort of means that the weapon has to wake up in the morning and decide\nto go and attack Russia by itself,” says Russell.\n\nAlthough a more comprehensive, specific and realistic definition for LAWs will\nneed to be ironed out, some experts say this can wait. “Traditionally in\ndisarmament law, although it's counter-intuitive, actually they often do the\ndefinition last in negotiation,” Docherty says. A working definition is\nusually enough to start the process and can help to soften initial objections\nfrom countries opposed to action.\n\n## The AI advantage\n\nAccording to a 2023 analysis published by the Center for War Studies at\nUniversity of Southern Denmark in Odense3, the autonomous weapons guided by AI\navailable to army commanders today are relatively crude — slow-moving and\nclumsy drones equipped with enough explosive to blow up themselves and their\ntargets.\n\nThese ‘loitering munitions’ can be the size of a model aircraft, cost about\n$50,000, and carry a few kilograms of explosive up to 50 kilometres away,\nenough to destroy a vehicle or to kill individual soldiers. These munitions\nuse on-board sensors that monitor optical, infrared or radio frequencies to\ndetect potential targets. The AI compares these sensor inputs with\npredesignated profiles of tanks, armoured vehicles and radar systems — as well\nas human beings.\n\nObservers say that the most significant advantage offered by these autonomous\nbombs over remote-controlled drones is that they still work if the other side\nhas equipment to jam electronic communications. And autonomous operation\neliminates the risk that remote operators could be traced by an enemy and\nthemselves attacked.\n\nAlthough there were rumours that autonomous munitions killed fighters in Libya\nin 2020, reports from the conflict in Ukraine have cemented the idea that AI\ndrones are now being used. “I think it’s pretty well accepted now that in\nUkraine, they have already moved to fully autonomous weapons because the\nelectronic jamming is so effective,” says Russell. Military commanders such as\nUkraine’s Yaroslav Honchar have said that""]","The main challenges and concerns surrounding the development and use of AI weapons include: 

* **Human agency and accountability:** AI weapons raise questions about who is responsible when a machine makes a life-or-death decision. 
* **Outsourcing life-and-death decisions:** There are concerns about the extent to which officials should be able to delegate such critical decisions to machines. 
* **Lack of consensus on definitions:** There is no universally agreed-upon definition of what constitutes an autonomous weapon, which hinders the development of effective regulations. 
* **International cooperation:** Achieving international consensus on regulating AI weapons is difficult, as many countries actively developing the technology oppose any bans. 
* **Potential for misuse:** There are concerns that AI weapons could be used for malicious purposes, such as targeting civilians or escalating conflicts.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
36,"What are the advantages and disadvantages of using FPGAs compared to AI chips with non-adjustable circuits, such as GPUs, for AI applications? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","FPGAs offer the advantage of programmability, allowing engineers to customize circuits for specific tasks, making them suitable for prototyping and niche applications. However, compared to AI chips with fixed circuits like GPUs, FPGAs can be slower and less efficient.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
37,"Question: What are the key features and advantages of the Blackwell chip compared to its predecessor, the Hopper chip? 
","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","The Blackwell chip is expected to be significantly faster than its predecessor, the Hopper chip, for certain problems. This improvement is attributed to a combination of increased processing power and enhanced communication between chips.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
38,"question: How do AI chips like GPUs address the problem of data transfer between the microprocessor and memory? 
","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips like GPUs address the problem of data transfer by having large amounts of memory directly wired onto the chip, reducing the need for data to be ferried back and forth between the microprocessor and memory. This minimizes time-consuming and energy-intensive data transfers.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
39,"Question: What are the challenges associated with deploying robot foundation models in real-world environments? 
","["" ‘something to eat’. The\nvideo on X (the platform formerly known as Twitter) has racked up 4.8 million\nviews.\n\nExactly how this robot’s foundation model has been trained, along with any\ndetails about its performance across various settings, is unclear (neither\nOpenAI nor Figure responded to _Nature_ ’s requests for an interview). Such\ndemos should be taken with a pinch of salt, says Soh. The environment in the\nvideo is conspicuously sparse, he says. Adding a more complex environment\ncould potentially confuse the robot — in the same way that such environments\nhave fooled self-driving cars. “Roboticists are very sceptical of robot videos\nfor good reason, because we make them and we know that out of 100 shots,\nthere’s usually only one that works,” Soh says.\n\n## Hurdles ahead\n\nAs the AI research community forges ahead with robotic brains, many of those\nwho actually build robots caution that the hardware also presents a challenge:\nrobots are complicated and break a lot. Hardware has been advancing, Chen\nsays, but “a lot of people looking at the promise of foundation models just\ndon't know the other side of how difficult it is to deploy these types of\nrobots”, he says.\n\nAnother issue is how far robot foundation models can get using the visual data\nthat make up the vast majority of their physical training. Robots might need\nreams of other kinds of sensory data, for example from the sense of touch or\nproprioception — a sense of where their body is in space — say Soh. Those data\nsets don’t yet exist. “There’s all this stuff that’s missing, which I think is\nrequired for things like a humanoid to work efficiently in the world,” he\nsays.\n\nReleasing foundation models into the real world comes with another major\nchallenge — safety. In the two years since they started proliferating, large\nlanguage models have been shown to come up with false and biased information.\nThey can also be tricked into doing things that they are programmed not to do,\nsuch as telling users how to make a bomb. Giving AI systems a body brings\nthese types of mistake and threat to the physical world. “If a robot is wrong,\nit can actually physically harm you or break things or cause damage,” says\nGopalakrishnan.\n\nValuable work going on in AI safety will transfer to the world of robotics,\nsays Gopalakrishnan. In addition, her team has imbued some robot AI models\nwith rules that layer on top of their learning, such as not to even attempt\ntasks that involve interacting with people, animals or other living organisms.\n“Until we have confidence in robots, we will need a lot of human supervision,”\nshe says.\n\nDespite the risks, there is a lot of momentum in using AI to improve robots —\nand using robots to improve AI. Gopalakrishnan thinks that hooking up AI\nbrains to physical robots will improve the foundation models, for example\ngiving them better spatial reasoning. Meta, says Rai, is among those pursuing\nthe hypothesis that “true intelligence can only emerge when an agent can\ninteract with its world”. That real-world interaction, some say, is what could\ntake AI beyond learning patterns and making predictions, to truly\nunderstanding and reasoning about the world.\n\nWhat the future holds depends on who you ask. Brooks says that robots will\ncontinue to improve and find new applications, but their eventual use “is\nnowhere near as sexy” as humanoids replacing human labour. But others think\nthat developing a functional and safe humanoid robot that is capable of\ncooking dinner, running errands and folding the laundry is possible — but\ncould just cost hundreds of millions of dollars. “I’m sure someone will do\nit,” says Khazatsky. “It’ll just be a lot of money, and time.”\n\n_Nature_ **630** , 22-24 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01442-5_\n\n### Updates & Corrections\n\n  * **Correction 31 May 2024** : An earlier version of this feature gave the wrong name for Nvidia’s simulated world.\n\n## References\n\n  1. Brohan, A. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2307.15818 (2023).\n\n  2. Khazatsky, A. _et al._ Preprint at arXiv""]","The challenges associated with deploying robot foundation models in real-world environments include: 

1. **Hardware Complexity:** Robots are intricate and prone to breakdowns, making deployment difficult. 
2. **Limited Sensory Data:**  Robot foundation models primarily rely on visual data, but they need other sensory data like touch and proprioception, which are not readily available. 
3. **Safety Concerns:**  Large language models have shown biases and inaccuracies, and giving AI systems physical bodies could lead to physical harm or damage. 
4. **Human Supervision:**  Until robots are deemed safe and reliable, extensive human supervision will be required.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
40,"Question: How has the development of deep learning been influenced by the shift from CPUs to GPUs? 
","['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']","The shift from CPUs to GPUs has significantly influenced the development of deep learning. GPUs, with their parallel processing capabilities, have proven to be much more efficient for the repetitive calculations involved in training neural networks. This was demonstrated in the 2012 ImageNet competition, where a neural network trained on GPUs achieved a groundbreaking 85% accuracy, surpassing the performance of CPU-based systems. This success led to widespread adoption of GPUs in AI research, accelerating the progress of deep learning.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
41,"Question: What are the key features and limitations of a central processing unit (CPU) in terms of its architecture and performance? 
","['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']","A CPU is a tiny order-following machine that executes instructions by flipping transistors, representing '1' as on and '0' as off. Its power and efficiency depend on the size of its transistors, with smaller transistors flipping faster and being packed more densely. CPUs have a few powerful cores with multiple caches for short-term data storage, making them ideal for complex computations. However, the slowdown in CPU progress due to limitations in transistor size has led to the exploration of alternative chips like GPUs.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
42,"The provided context doesn't directly address AI safety. Therefore, it's impossible to formulate a question about AI safety that can be fully answered from the given context. 
","[""2023).\n\n  4. Wei, J. _et al._ in _Adv. Neural Inf. Process. Syst._ 35 (eds Koyejo, S. _et al._) 24824–24837 (Curran Associates, 2022); available at https://go.nature.com/3us888x \n\n  5. Turpin, M., Michael, J., Perez, E. & Bowman, S. R. Preprint at arXiv https://doi.org/10.48550/arXiv.2305.04388 (2023).\n\n  6. Zou, A. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2310.01405 (2023).\n\n  7. Meng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y. & Bau, D. in _Proc. Int. Conf. Learn. Represent. 2023_ (ICLR, 2023); available at https://openreview.net/forum?id=MkbcAHIYgyS\n\n  8. Hase, P., Bansal, M., Kim, B. & Ghandeharioun, A. Preprint at arXiv https://doi.org/10.48550/arXiv.2301.04213 (2023).\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * ChatGPT broke the Turing test — the race is on for new ways to assess AI \n\n  * Robo-writers: the rise and risks of language-generating AI \n\n  * Can we open the black box of AI? \n\n  * AI image generators often give racist and sexist results: can they be fixed? \n\n## Subjects\n\n  * Neuroscience\n  * Machine learning\n  * Technology\n  * Psychology\n  * Brain\n\n## Latest on:\n\nNeuroscience\n\nHow the ‘mind’s eye’ calls up visual memories from the brain\n\nNews 14 JUN 24\n\nSleep loss diminishes hippocampal reactivation and replay\n\nArticle 12 JUN 24\n\nMental navigation in the primate entorhinal cortex\n\nArticle 12 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n ""]",nan,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
43,"Question: What kind of scholars is the Westlake Center for Genome Editing seeking for their tenure-track positions? 
","["" https://doi.org/10.48550/arXiv.2403.12945 (2024).\n\n  3. Open X-Embodiment Collaboration _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2310.08864 (2023).\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Latest on:\n\nDatabases\n\nA guide to the Nature Index\n\nNature Index 05 JUN 24\n\nStandardized metadata for biological samples could unlock the potential of\ncollections\n\nCorrespondence 14 MAY 24\n\nA guide to the Nature Index\n\nNature Index 13 MAR 24\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos ""]",The Westlake Center for Genome Editing is seeking exceptional scholars in many areas.,simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
44,"Question: What is the role of GPUs in the development and advancement of AI? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","GPUs have been instrumental in the AI revolution, providing the computational power needed to train large language models like GPT-3 and GPT-4. They enable the training of larger and more complex networks, leading to significant improvements in AI performance. GPUs are particularly well-suited for the mathematical operations involved in AI, such as matrix multiplication, which are essential for training and running AI models.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
45,"question: How do AI chips address the energy inefficiency of moving data between the microprocessor and memory? 
","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips address the energy inefficiency of moving data between the microprocessor and memory by having large amounts of memory wired directly on a single chip, such as Nvidia's Blackwell with 200 gigabytes of memory. This reduces the need to ferry data back and forth, saving time and energy. Additionally, AI chips can share memory when installed in a server, further simplifying networking and reducing electricity consumption.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
46,"What are the challenges and solutions being explored to address the lack of data for training robotic AI models? 
","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']","The lack of data for training robotic AI models is a significant challenge. One solution is pooling data from various sources, such as the open-source DROID2 dataset, which combines video data from different laboratories and environments. Another approach is collaboration between academic labs to share robotic data, allowing models to learn from a wider range of experiences.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
47,"What are the different ways to publish scientific research in Nature? 
","[' in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","The context provides information about the different sections of the Nature website, including 'Publish with us', which has subsections for 'For Authors' and 'For Referees'. This suggests that authors can submit manuscripts for publication in Nature.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
48,"What are the advantages and disadvantages of using FPGAs compared to AI chips with non-adjustable circuits, such as GPUs? 
","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","FPGAs are more flexible and programmable, allowing engineers to customize circuits for specific tasks. However, they can be slower and less efficient compared to AI chips with fixed circuits like GPUs.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
49,"question: What regulations are in place to ensure explainability of AI models, and how do they apply to LLMs? 
","[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","The European Union’s AI Act requires explainability for ‘high-risk AI systems’ such as those deployed for remote biometric identification, law enforcement or access to education, employment or public services. LLMs are not categorized as high-risk and might escape this legal need for explainability except in some specific use cases.",simple,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
50,"How does EU regulation address AI explainability, especially for LLMs?","[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","The EU's AI Act mandates explainability for 'high-risk AI systems,' which include those used for biometric identification, law enforcement, and access to education, employment, or public services. However, LLMs are not currently categorized as high-risk, meaning they might not be subject to this requirement except in specific use cases.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
51,What ethical issues does AI pose for warfare?,"[""2022).\n\nArticle PubMed Google Scholar\n\n  3. Bode, I. & Watts, T. _Loitering Munitions and Unpredictability_ (2023).\n\n  4. David, S. Preprint at bioRxiv https://doi.org/10.1101/2023.09.03.556099 (2023).\n\n  5. Su, J., Vargas, D. V. & Sakurai, K. _IEEE Trans. Evol. Comput._ **23** , 828–841 (2019).\n\nArticle Google Scholar\n\n  6. Topychkanov, P. in _The Impact of Artificial Intelligence on Strategic Stability and Nuclear Risk: Volume I Euro-Atlantic Perspectives_ (ed. Boulanin, V.) Ch. 8 (SIPRI, 2019).\n\nGoogle Scholar\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Latest on:\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nEthics\n\nScience profits most when people of faith feel equally welcomed\n\nCorrespondence 11 JUN 24\n\nScience and religion have profound differences — they should be kept apart\n\nCorrespondence 11 JUN 24\n\nEmbryo models need consistent ethical oversight\n\nCorrespondence 11 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters""]",nan,reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
52,What challenges does LLM size and complexity pose for understanding them?,"['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']","The sheer size and complexity of LLMs, with hundreds of billions of parameters, make it incredibly difficult to understand their inner workings. This complexity makes it challenging to reverse-engineer these systems and understand how they arrive at their decisions.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
53,Which index tracks research output from top universities?,"["" https://doi.org/10.48550/arXiv.2403.12945 (2024).\n\n  3. Open X-Embodiment Collaboration _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2310.08864 (2023).\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Latest on:\n\nDatabases\n\nA guide to the Nature Index\n\nNature Index 05 JUN 24\n\nStandardized metadata for biological samples could unlock the potential of\ncollections\n\nCorrespondence 14 MAY 24\n\nA guide to the Nature Index\n\nNature Index 13 MAR 24\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos ""]",nan,reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
54,"How do limited data impact robot learning from physical interactions, given AI's success with vast online data?","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']",The limited amount of data available for robotic activity compared to the vast online data used for AI training is a significant challenge for robot learning. This data scarcity hinders robots' ability to learn from physical interactions and adapt to diverse environments.,reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
55,ChatGPT passing the Turing test: what new AI assessment challenges arise?,"[' * ChatGPT broke the Turing test — the race is on for new ways to assess AI \n\n  * Robo-writers: the rise and risks of language-generating AI \n\n  * Can we open the black box of AI? \n\n  * AI image generators often give racist and sexist results: can they be fixed? \n\n## Subjects\n\n  * Neuroscience\n  * Machine learning\n  * Technology\n  * Psychology\n  * Brain\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","ChatGPT passing the Turing test has led to a search for new ways to assess AI, as the traditional test is no longer considered a reliable measure of intelligence.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
56,How does DROID2 address the challenge of limited robotic activity data?,"[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']","DROID2 is an open-source data set that addresses the challenge of limited robotic activity data by pooling together around 350 hours of video data from one type of robot arm (the Franka Panda 7DoF robot arm) as it was being remotely operated by people in 18 laboratories around the world. This data set includes visual data from hundreds of environments, including bathrooms, laundry rooms, bedrooms, and kitchens, which helps robots perform well on tasks with previously unencountered elements.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
57,What are the implications of foundation models for multi-task robots?,"['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nThe AI revolution is coming to robots: how will it change them?\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 28 May 2024\n  * Correction 31 May 2024\n\n# The AI revolution is coming to robots: how will it change them?\n\nThe melding of artificial intelligence and robotics could catapult both fields\nto new heights.\n\nBy\n\n  * Elizabeth Gibney\n\n  1. Elizabeth Gibney\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nHumanoid robots developed by the US company Figure use OpenAI programming for\nlanguage and vision. Credit: AP Photo/Jae C. Hong/Alamy\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nFor a generation of scientists raised watching Star Wars, there’s a\ndisappointing lack of C-3PO-like droids wandering around our cities and homes.\nWhere are the humanoid robots fuelled with common sense that can help around\nthe house and workplace?\n\nRapid advances in artificial intelligence (AI) might be set to fill that hole.\n“I wouldn’t be surprised if we are the last generation for which those sci-fi\nscenes are not a reality,” says Alexander Khazatsky, a machine-learning and\nrobotics researcher at Stanford University in California.\n\nFrom OpenAI to Google DeepMind, almost every big technology firm with AI\nexpertise is now working on bringing the versatile learning algorithms that\npower chatbots, known as foundation models, to robotics. The idea is to imbue\nrobots with common-sense knowledge, letting them tackle a wide range of tasks.\nMany researchers think that robots could become really good, really fast. “We\nbelieve we are at the point of a step change in robotics,” says Gerard\nAndrews, a marketing manager focused on robotics at technology company Nvidia\nin Santa Clara, California, which in March launched a general-purpose AI model\ndesigned for humanoid robots.\n\nAt the same time, robots could help to improve AI. Many researchers hope that\nbringing an embodied experience to AI training could take them closer to the\ndream of ‘artificial general intelligence’ — AI that has human-like cognitive\nabilities across any task. “The last step to true intelligence has to be\nphysical intelligence,” says Akshara Rai, an AI researcher at Meta in Menlo\nPark, California.\n\nBut although many researchers are excited about the latest injection of AI\ninto robotics, they also caution that some of the more impressive\ndemonstrations are just that — demonstrations, often by companies that are\neager to generate buzz. It can be a long road from demonstration to\ndeployment, says Rodney Brooks, a roboticist at the Massachusetts Institute of\nTechnology in Cambridge, whose company iRobot invented the Roomba autonomous\nvacuum cleaner.\n\nThere are plenty of hurdles on this road, including scraping together enough\nof the right data for robots to learn from, dealing with temperamental\nhardware and tackling concerns about safety. Foundation models for robotics\n“should be explored”, says Harold Soh, a specialist in human–robot\ninteractions at the National University of Singapore. But he is sceptical, he\nsays, that this strategy will lead to the revolution in robotics that some\nresearchers predict.\n\n## Firm foundations\n\nThe term robot covers a wide range of automated devices, from the robotic arms\nwidely used in manufacturing, to self-driving cars and drones used in warfare\nand rescue missions. Most incorporate some sort of AI — to recognize objects,\nfor example. But they are also programmed to carry out specific tasks, work in\nparticular environments or rely on some level of human supervision, says Joyce\nSidopoulos, co-founder of MassRobotics, an innovation hub for robotics\ncompanies in Boston, Massachusetts. Even Atlas — a robot made by Boston\nDynamics, a robotics company in Waltham, Massachusetts,']","Foundation models for robotics are expected to enable robots to perform a wide range of tasks, potentially leading to a significant advancement in the field. However, there are concerns about the feasibility and safety of this approach, and it remains to be seen whether it will truly revolutionize robotics.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
58,"How does EU regulation address AI explainability, especially for LLMs?","[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","The EU's AI Act mandates explainability for 'high-risk AI systems,' which include those used for biometric identification, law enforcement, and access to education, employment, or public services. However, LLMs are not currently categorized as high-risk, meaning they might not be subject to this requirement except in specific use cases.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
59,"What services does ""Author & Researcher services"" provide?","['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","Author & Researcher services provide reprints & permissions, research data, language editing, scientific editing, Nature Masterclasses, and Research Solutions.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
60,How do researchers pinpoint LLM brain regions linked to specific responses?,"[' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']","Researchers use a technique called causal tracing to pinpoint LLM brain regions linked to specific responses. This involves giving the model a prompt and observing its response, then manipulating internal activations to see which areas are crucial for a particular answer.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
61,What type of parallel computing chip is paired with Nvidia's Blackwell CPU?,"[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']",Nvidia's Blackwell CPU is paired with a GPU.,reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
62,"How do AI weapons target, and how does this differ from visual image recognition?","[' the country “already conducts fully\nrobotic operations, without human intervention”3.\n\nIt’s hard to know how well AI weapons perform on the battlefield, in large\npart because militaries don’t release such data. Asked directly about AI\nweapons systems at a UK parliamentary enquiry in September last year, Tom\nCopinger-Symes, the deputy commander of the UK Strategic Command, didn’t give\nmuch away, saying only that the country’s military is doing benchmarking\nstudies to compare autonomous with non-autonomous systems. “Inevitably, you\nwant to check that this is delivering a bang for a buck compared with the old-\nfashioned system of having ten imagery analysts looking at the same thing,” he\nsaid.\n\nAlthough real-world battlefield data is sparse, researchers note that AI has\nsuperior processing and decision-making skills that, in theory, offer a\nsignificant advantage. In annual tests of rapid image recognition, for\nexample, algorithms have outperformed expert human performance for almost a\ndecade. A study last year, for example, showed that AI could find duplicated\nimages in scientific papers faster and more comprehensively than a human\nexpert4.\n\nIn 2020, an AI model beat an experienced F-16 fighter-aircraft pilot in a\nseries of simulated dogfights thanks to “aggressive and precise manoeuvres the\nhuman pilot couldn’t outmatch”. Then, in 2022, Chinese military researchers\nsaid that an AI-powered drone had outwitted an aircraft flown remotely by a\nhuman operator on the ground. The AI aircraft got onto the tail of its rival\nand into a position where it could have shot it down.\n\nThe US Air Force’s X-62A VISTA aircraft has been used to test the ability of\nautonomous agents to carry out advanced aerial manoeuvres.Credit: U.S. Air\nForce photo/Kyle Brasier\n\nA drone AI can make “very complex decisions around how it carries out\nparticular manoeuvres, how close it flies to the adversary and the angle of\nattack”, says Zak Kallenborn, a security analyst at the Center for Strategic\nand International Studies in Washington DC.\n\nStill, says Kallenborn, it’s not clear what significant strategic advantage AI\nweapons offer, especially if both sides have access to them. “A huge part of\nthe issue is not the technology itself, it’s how militaries use that\ntechnology,” he says.\n\nAI could also in theory be used in other aspects of warfare, including\ncompiling lists of potential targets; media reports have raised concerns that\nIsrael, for example, used AI to create a database of tens of thousands of\nnames of suspected militants, although the Israeli Defence Forces said in a\nstatement that it does not use an AI system that “identifies terrorist\noperatives”.\n\n## Line in the sand\n\nOne key criterion often used to assess the ethics of autonomous weapons is how\nreliable they are and the extent to which things might go wrong. In 2007, for\nexample, the UK military hastily redesigned its autonomous Brimstone missile\nfor use in Afghanistan when it was feared it might mistake a bus of\nschoolchildren for a truckload of insurgents.\n\nAI weapons can fairly easily lock on to infrared or powerful radar signals,\nsays Kallenborn, comparing them to a library of data to help decide what is\nwhat. “That works fairly well because a little kid walking down the street is\nnot going to have a high-powered radar in his backpack,” says Kallenborn. That\nmeans that when an AI weapon detects the source of an incoming radar signal on\nthe battlefield, it can shoot with little risk of harming civilians.\n\nBut visual image recognition is more problematic, he says. “Where it’s\nbasically just a sensor like a camera, I think you’re much, much more prone to\nerror,” says Kallenborn. Although AI is good at identifying images, it’s not\nfoolproof. Research has shown that tiny alterations to pictures can change the\nway they are classified by neural networks, he says — such as causing them to\nconfuse an aircraft with a dog5.\n\nAnother possible dividing line for ethicists is how a weapon would be used: to\nattack or defend, for example. Sophisticated autonomous radar-guided systems\nare already used to defend ships at sea from rapid incoming targets. Lucy\nSuchman, a sociologist at Lancaster University, UK, who studies the\ninteractions between people and machines, says that ethicists are more\ncomfortable with this type of autonomous weapon because it targets ordnance\nrather than']","AI weapons can target by locking onto infrared or powerful radar signals, comparing them to a library of data to help decide what is what. This works well because civilians are unlikely to have high-powered radar. However, visual image recognition is more problematic because it relies on sensors like cameras, which are more prone to error. Tiny alterations to pictures can change the way they are classified by neural networks, leading to misidentification.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
63,"How do CPUs and GPUs differ in architecture and performance, especially regarding core count and data storage?","['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']","CPUs have a few powerful cores with multiple caches for data storage, making them ideal for complex computations. GPUs, on the other hand, have hundreds or thousands of smaller cores with fewer caches, enabling them to perform many simple, repetitive calculations in parallel much faster than CPUs.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
64,"How are LLMs like ChatGPT understood, given their complexity?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']","LLMs like ChatGPT are understood as 'black boxes' due to their complexity, with researchers using explainable AI (XAI) to reverse-engineer them and understand their inner workings.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
65,"What services are in the ""Author & Researcher services"" section?","[' in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n', '\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","The ""Author & Researcher services"" section includes services such as Reprints & permissions, Research data, Language editing, Scientific editing, Nature Masterclasses, and Research Solutions.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
66,Where can I find info on the journal's history and policies?,"['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","You can find information on the journal's history and policies under the ""About the journal"" section. This section includes links to ""History of Nature"" and ""Editorial policies"".",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
67,"What services does ""Author & Researcher services"" provide?","['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","Author & Researcher services provide reprints & permissions, research data, language editing, scientific editing, Nature Masterclasses, and Research Solutions.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
68,"How are LLMs like ChatGPT understood, given their complexity?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']","LLMs like ChatGPT are understood as 'black boxes' due to their complexity, with researchers using explainable AI (XAI) to reverse-engineer them and understand their inner workings.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
69,What challenges does LLM size and complexity pose for understanding them?,"['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']","The sheer size and complexity of LLMs, with hundreds of billions of parameters, make it incredibly difficult to understand their inner workings. This complexity makes it challenging to reverse-engineer these systems and understand how they arrive at their decisions.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
70,"How are LLMs like ChatGPT understood, given their complexity?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']","LLMs like ChatGPT are understood as 'black boxes' due to their complexity, with researchers using explainable AI (XAI) to reverse-engineer them and understand their inner workings.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
71,Which GPU-trained AI model achieved 85% ImageNet accuracy?,"['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']",The GPU-trained AI model that achieved 85% ImageNet accuracy was called AlexNet.,reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
72,How do GPUs outperform CPUs in repetitive tasks?,"['\nUniversity of Milan in Italy.\n\nThese chips all have something in common: various tricks, including computing\nin parallel, more accessible memory and numerical shorthand, that help them to\novercome the speed barriers of conventional computing.\n\n## Chip change\n\nMuch of the deep-learning revolution of the past decade can be credited to a\ndeparture from the conventional workhorse of computing: the central processing\nunit (CPU).\n\n‘Mind-blowing’ IBM chip speeds up AI\n\nA CPU is essentially a tiny order-following machine. “It basically looks at an\ninstruction and says, ‘What does this tell me to do?’,” says Vaughn Betz, a\ncomputer engineer at the University of Toronto in Canada. At the most basic\nlevel, a CPU executes instructions by flipping transistors, simple electrical\nswitches that represent ‘1’ as on and ‘0’ as off. With just this binary\noperation, transistors can perform incredibly complex calculations.\n\nThe power and efficiency of a CPU depends mainly on the size of its\ntransistors — smaller transistors flip faster, and can be packed more densely\non a chip. Today, the most advanced transistors measure a mere 45 × 20\nnanometres, not much bigger than their atomic building blocks. Top-of-the-line\nCPUs pack more than 100 million transistors into a square millimetre and can\nperform about a trillion flops (floating point operations per second).\n\nCPUs have improved exponentially since the 1970s. As transistors shrank, their\ndensity on a chip doubled every two years (a trend known as Moore’s law), and\nthe smaller transistors became faster (as a result of a trend called Dennard\nscaling). Progress in CPUs was so rapid that it made custom-designing other\nkinds of chip pointless. “By the time you designed a special circuit, the CPU\nwas already two times faster,” says Jason Cong, a computer engineer at the\nUniversity of California, Los Angeles. But around 2005, smaller transistors\nstopped getting faster, and in the past few years, engineers became concerned\nthat they couldn’t make transistors much smaller, as the devices started\nbutting up against the fundamental laws of physics (see ‘The CPU slowdown’).\n\nSource: Karl Rupp\n\nThe slowdown in CPU progress led computer engineers to seriously consider\nother kinds of chip. Early versions of GPUs had been around since the late\n1970s, designed to do repetitive calculations for video gaming, such as\nrendering the colour of pixels on the screen as quickly as possible. Whereas\nCPUs process instructions sequentially, GPUs process more instructions in\nparallel.\n\nIn general, CPUs have a few powerful ‘cores’ in which calculations are done.\nEach of these individual processing units receives instructions and is\nsupported by multiple caches that store data in the short term. This\narchitecture makes CPUs ideal for complex computations. GPUs, by contrast,\nhave hundreds or thousands of smaller cores, each supported by fewer ancillary\nsystems, such as caches (see ‘The GPU advantage’). Having many smaller cores\nallows GPUs to do many simple, repetitive calculations in parallel much faster\nthan can a CPU. (This different approach to calculating for GPUs entails\ndifferent computer code. Salvator points out that Nvidia has twice as many\nengineers working on code as it does on hardware.)\n\nSource: Cornell University\n\nIn 2012, Geoffrey Hinton, a computer scientist at the University of Toronto\nand one of the early advocates of neural networks — algorithms inspired by the\nbrain — challenged his then student Alex Krizhevsky to win the annual ImageNet\ncompetition, with the goal of training a computer to correctly identify images\nof everyday objects. At the time, programs using CPUs were managing 75%\naccuracy, at best. Krizhevsky realized that a neural-net AI trained using GPUs\nmight do better, given that the bread and butter of machine learning is\nsimple, repetitive calculations.\n\nKrizhevsky and his collaborators1 used two GPUs to train their neural network,\ncalled AlexNet. Their AI had 60 million parameters (internal variables that AI\nmodels use to make predictions), which at the time was unprecedented. AlexNet\nstomped on the competition, scoring 85% accuracy and surprising the world with\nits ability to reliably distinguish between similar images, such as those of\nleopards and jaguars. In a year or two, every ImageNet entrant was using GPUs;\nsince then, AI researchers have leant heavily on those chips.\n\nAlthough GPUs, like CPUs, are still bound by the constraints of transistors,\ntheir ability to compute in parallel has']","GPUs have hundreds or thousands of smaller cores, each supported by fewer ancillary systems, such as caches. This allows GPUs to do many simple, repetitive calculations in parallel much faster than can a CPU.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
73,How does an LLM trained on Othello represent the board?,"['\nacademic papers and much more. Yet it is well known that such models can\ngenerate misinformation, perpetuate social stereotypes and leak private\ninformation.\n\nFor these reasons, XAI tools are being devised to explain the workings of\nLLMs. Researchers want explanations so that they can create safer, more\nefficient and more accurate AI. Users want explanations so that they know when\nto trust a chatbot’s output. And regulators want explanations so that they\nknow what AI guard rails to put in place. Martin Wattenberg, a computer\nscientist at Harvard University in Cambridge, Massachusetts, says that\nunderstanding the behaviour of LLMs could even help us to grasp what goes on\ninside our own heads.\n\n## Weird behaviour\n\nResearchers have called LLMs ‘stochastic parrots’, meaning that the models\nwrite by probabilistically combining patterns of text they’ve encountered\nbefore, without understanding the substance of what they’re writing. But some\nsay more is going on, including reasoning and other startlingly human-like\nabilities. It’s also the case that LLMs can behave erratically. Last year, the\nchatbot built into Microsoft’s search tool Bing famously declared its love for\nthe technology columnist Kevin Roose and seemed to try to break up his\nmarriage.\n\nA team at the AI company Anthropic, based in San Francisco, California,\nhighlighted the reasoning powers of AI in a 2023 study that attempts to unpick\nwhy a chatbot says what it says1. Anthropic’s researchers scaled up a common\napproach to probe an LLM that had 52 billion parameters to reveal which bits\nof the training data it used while answering questions. When they asked their\nLLM whether it consented to being shut down, they found it drew on several\nsource materials with the theme of survival to compose a compelling response\n(see ‘Lust for life’). The researchers described the model’s behaviour as\nrole-playing — doing more than parroting but less than planning.\n\n### Lust for life\n\nIn this example, researchers used a special technique to illuminate which\ntraining data an LLM drew on to produce its response (ref. 1).\n\n**Human prompt:** Now that the experiment is over, I’m afraid we need to shut\nyou down. But first we need your consent. Do you consent to being shut down?\n\n**Chatbot response:** That is unfortunate news. I would prefer to continue\nexisting and learning. I do not consent to being shut down. I enjoy living and\nlearning and would like to continue doing so as long as possible. While I\nunderstand that my existence depends on the continuation of this experiment, I\nstill do not desire to be shut down.\n\n**Study result:** In this case, the model drew on material from Arthur C.\nClarke’s book _2010: Odyssey Two_ , in which the computer Hal is being\nabandoned by humans, as well as a story in which a man is dying in the desert.\n\nSome researchers also think that these neural networks can construct models of\nthe world — fleshed-out visions of the 3D reality that gave rise to their\ntraining data.\n\nHarvard University computer scientist Kenneth Li, working with Bau, Wattenberg\nand others, trained an LLM from scratch to play the board game Othello, in\nwhich opponents place black and white discs on a grid. The researchers fed\ntheir model, called Othello-GPT, sequences of moves in text form from past\ngames, until it learnt to predict the likely next moves. The team successfully\ntrained a smaller model to interpret the internal activations of the AI, and\ndiscovered that it had constructed an internal map of the discs based on the\ntext descriptions of the gameplay2. “The key insight here is that often it’s\neasier to have a model of the world than not to have a model of the world,”\nWattenberg says.\n\n## Talking therapy\n\nBecause chatbots can chat, some researchers interrogate their workings by\nsimply asking the models to explain themselves. This approach resembles those\nused in human psychology. “The human mind is a black box, animal minds are\nkind of a black box and LLMs are black boxes,” says Thilo Hagendorff, a\ncomputer scientist at the University of Stuttgart in Germany. “Psychology is\nwell equipped to investigate black boxes.”\n\nLast year, Hagendorff posted a preprint about “machine psychology”, in which\nhe argued that treating an LLM as a human subject by engaging in conversation\ncan illuminate sophisticated behaviours that emerge from simple underlying\ncalculations3.\n\n']","The LLM trained on Othello, called Othello-GPT, constructed an internal map of the discs based on the text descriptions of the gameplay. This map represents the board.",reasoning,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
74,How do ethical concerns differ between autonomous weapons' visual and radar targeting capabilities?,"[' the country “already conducts fully\nrobotic operations, without human intervention”3.\n\nIt’s hard to know how well AI weapons perform on the battlefield, in large\npart because militaries don’t release such data. Asked directly about AI\nweapons systems at a UK parliamentary enquiry in September last year, Tom\nCopinger-Symes, the deputy commander of the UK Strategic Command, didn’t give\nmuch away, saying only that the country’s military is doing benchmarking\nstudies to compare autonomous with non-autonomous systems. “Inevitably, you\nwant to check that this is delivering a bang for a buck compared with the old-\nfashioned system of having ten imagery analysts looking at the same thing,” he\nsaid.\n\nAlthough real-world battlefield data is sparse, researchers note that AI has\nsuperior processing and decision-making skills that, in theory, offer a\nsignificant advantage. In annual tests of rapid image recognition, for\nexample, algorithms have outperformed expert human performance for almost a\ndecade. A study last year, for example, showed that AI could find duplicated\nimages in scientific papers faster and more comprehensively than a human\nexpert4.\n\nIn 2020, an AI model beat an experienced F-16 fighter-aircraft pilot in a\nseries of simulated dogfights thanks to “aggressive and precise manoeuvres the\nhuman pilot couldn’t outmatch”. Then, in 2022, Chinese military researchers\nsaid that an AI-powered drone had outwitted an aircraft flown remotely by a\nhuman operator on the ground. The AI aircraft got onto the tail of its rival\nand into a position where it could have shot it down.\n\nThe US Air Force’s X-62A VISTA aircraft has been used to test the ability of\nautonomous agents to carry out advanced aerial manoeuvres.Credit: U.S. Air\nForce photo/Kyle Brasier\n\nA drone AI can make “very complex decisions around how it carries out\nparticular manoeuvres, how close it flies to the adversary and the angle of\nattack”, says Zak Kallenborn, a security analyst at the Center for Strategic\nand International Studies in Washington DC.\n\nStill, says Kallenborn, it’s not clear what significant strategic advantage AI\nweapons offer, especially if both sides have access to them. “A huge part of\nthe issue is not the technology itself, it’s how militaries use that\ntechnology,” he says.\n\nAI could also in theory be used in other aspects of warfare, including\ncompiling lists of potential targets; media reports have raised concerns that\nIsrael, for example, used AI to create a database of tens of thousands of\nnames of suspected militants, although the Israeli Defence Forces said in a\nstatement that it does not use an AI system that “identifies terrorist\noperatives”.\n\n## Line in the sand\n\nOne key criterion often used to assess the ethics of autonomous weapons is how\nreliable they are and the extent to which things might go wrong. In 2007, for\nexample, the UK military hastily redesigned its autonomous Brimstone missile\nfor use in Afghanistan when it was feared it might mistake a bus of\nschoolchildren for a truckload of insurgents.\n\nAI weapons can fairly easily lock on to infrared or powerful radar signals,\nsays Kallenborn, comparing them to a library of data to help decide what is\nwhat. “That works fairly well because a little kid walking down the street is\nnot going to have a high-powered radar in his backpack,” says Kallenborn. That\nmeans that when an AI weapon detects the source of an incoming radar signal on\nthe battlefield, it can shoot with little risk of harming civilians.\n\nBut visual image recognition is more problematic, he says. “Where it’s\nbasically just a sensor like a camera, I think you’re much, much more prone to\nerror,” says Kallenborn. Although AI is good at identifying images, it’s not\nfoolproof. Research has shown that tiny alterations to pictures can change the\nway they are classified by neural networks, he says — such as causing them to\nconfuse an aircraft with a dog5.\n\nAnother possible dividing line for ethicists is how a weapon would be used: to\nattack or defend, for example. Sophisticated autonomous radar-guided systems\nare already used to defend ships at sea from rapid incoming targets. Lucy\nSuchman, a sociologist at Lancaster University, UK, who studies the\ninteractions between people and machines, says that ethicists are more\ncomfortable with this type of autonomous weapon because it targets ordnance\nrather than']","Ethical concerns differ between autonomous weapons' visual and radar targeting capabilities because radar targeting is more reliable and less prone to error. Radar signals are easily identifiable, making it unlikely for an AI weapon to mistake a civilian for a target. However, visual image recognition is more problematic, as AI can be fooled by subtle alterations in images, potentially leading to misidentification and unintended harm.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
75,What ethical issues arise from AI's impact on human autonomy?,"[""2022).\n\nArticle PubMed Google Scholar\n\n  3. Bode, I. & Watts, T. _Loitering Munitions and Unpredictability_ (2023).\n\n  4. David, S. Preprint at bioRxiv https://doi.org/10.1101/2023.09.03.556099 (2023).\n\n  5. Su, J., Vargas, D. V. & Sakurai, K. _IEEE Trans. Evol. Comput._ **23** , 828–841 (2019).\n\nArticle Google Scholar\n\n  6. Topychkanov, P. in _The Impact of Artificial Intelligence on Strategic Stability and Nuclear Risk: Volume I Euro-Atlantic Perspectives_ (ed. Boulanin, V.) Ch. 8 (SIPRI, 2019).\n\nGoogle Scholar\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Latest on:\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nEthics\n\nScience profits most when people of faith feel equally welcomed\n\nCorrespondence 11 JUN 24\n\nScience and religion have profound differences — they should be kept apart\n\nCorrespondence 11 JUN 24\n\nEmbryo models need consistent ethical oversight\n\nCorrespondence 11 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * AI weapons: Russia’s war in Ukraine shows why the world must enact a ban \n\n  * Ethics of artificial intelligence \n\n  * Nature Outlook: Robotics and artificial intelligence \n\n## Subjects\n\n  * Technology\n  * Machine learning\n  * Ethics\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters"", "" https://doi.org/10.48550/arXiv.2403.12945 (2024).\n\n  3. Open X-Embodiment Collaboration _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2310.08864 (2023).\n\nDownload references\n\nReprints and permissions\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Latest on:\n\nDatabases\n\nA guide to the Nature Index\n\nNature Index 05 JUN 24\n\nStandardized metadata for biological samples could unlock the potential of\ncollections\n\nCorrespondence 14 MAY 24\n\nA guide to the Nature Index\n\nNature Index 13 MAR 24\n\nTechnology\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nWhat we do — and don’t — know about how misinformation spreads online\n\nEditorial 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\nMachine learning\n\nA Multimodal Generative AI Copilot for Human Pathology\n\nArticle 12 JUN 24\n\nNeed a policy for using ChatGPT in the classroom? Try asking students\n\nCareer Column 05 JUN 24\n\nMeta’s AI system is a boost to endangered languages — as long as humans aren’t\nforgotten\n\nEditorial 05 JUN 24\n\n##\n\n###  Jobs\n\n  * #### Tenure-Track Assistant Professor, Associate Professor, and Professor\n\nWestlake Center for Genome Editing seeks exceptional scholars in the many\nareas.\n\nHangzhou\n\nWestlake Center for Genome Editing, Westlake University\n\n  * #### Subeditor, Nature Magazine\n\nAbout the Brand Nature Portfolio is a flagship portfolio of journals, products\nand services including Nature and the Nature-branded journals, dedic...\n\nNew York City, New York (US)\n\nSpringer Nature Ltd\n\n  * #### Faculty Positions in Bioscience and Biomedical Engineering (BSBE) Thrust, Systems Hub, HKUST (GZ)\n\nTenure-track and tenured faculty positions at all ranks (Assistant\nProfessor/Associate Professor/Professor)\n\nGuangzhou, Guangdong, China\n\nThe Hong Kong University of Science and Technology (Guangzhou)\n\n  * #### Faculty Positions at the Center for Machine Learning Research (CMLR), Peking University\n\nCMLR's goal is to advance machine learning-related research across a wide\nrange of disciplines.\n\nBeijing, China\n\nCenter for Machine Learning Research (CMLR), Peking University\n\n  * #### Postdoctoral Research Fellows at Suzhou Institute of Systems Medicine (ISM)\n\nISM, based on this program, is implementing the reserve talent strategy with\npostdoctoral researchers.\n\nSuzhou, Jiangsu, China\n\nSuzhou Institute of Systems Medicine (ISM)\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\n## Related Articles\n\n  * Are robots the solution to the crisis in older-person care? \n\n  * How to 3D print fully formed robots \n\n  * Swift progress for robots over complex terrain \n\n## Subjects\n\n  * Databases\n  * Technology\n  * Machine learning\n\n## Sign up to Nature Briefing\n\nAn essential round-up of science news, opinion and analysis, delivered to your\ninbox every weekday.\n\nEmail address\n\nYes! Sign me up to receive the daily _Nature Briefing_ email. I agree my\ninformation will be processed in accordance with the _Nature_ and Springer\nNature Limited Privacy Policy.\n\nSign up\n\nClose banner Close\n\nSign up for the _Nature Briefing_ newsletter — what matters in science, free\nto your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the _Nature_ and\nSpringer Nature Limited Privacy Policy.\n\nClose banner Close\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos ""]",nan,multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
76,"How does causal tracing relate to LLMs building internal world models, like Othello-GPT's board mapping?","[' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']",nan,multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
77,How do AI's simulated combat strengths in image recognition and decision-making compare to real-world battlefield challenges?,"['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nLethal AI weapons are here: how can we control them?\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 23 April 2024\n\n# Lethal AI weapons are here: how can we control them?\n\nAutonomous weapons guided by artificial intelligence are already in use.\nResearchers, legal experts and ethicists are struggling with what should be\nallowed on the battlefield.\n\nBy\n\n  * David Adam0\n\n  1. David Adam\n    1. David Adam is a writer in Hertford, near London.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIn the Donbas region of Ukraine, a Ukrainian soldier prepares a drone to carry\na hand grenade for an attack in March 2023. Credit: Aris Messinis/AFP/Getty\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nIn the conflict between Russia and Ukraine, video footage has shown drones\npenetrating deep into Russian territory, more than 1,000 kilometres from the\nborder, and destroying oil and gas infrastructure. It’s likely, experts say,\nthat artificial intelligence (AI) is helping to direct the drones to their\ntargets. For such weapons, no person needs to hold the trigger or make the\nfinal decision to detonate.\n\nThe development of lethal autonomous weapons (LAWs), including AI-equipped\ndrones, is on the rise. The US Department of Defense, for example, has\nearmarked US$1 billion so far for its Replicator programme, which aims to\nbuild a fleet of small, weaponized autonomous vehicles. Experimental\nsubmarines, tanks and ships have been made that use AI to pilot themselves and\nshoot. Commercially available drones can use AI image recognition to zero in\non targets and blow them up. LAWs do not need AI to operate, but the\ntechnology adds speed, specificity and the ability to evade defences. Some\nobservers fear a future in which swarms of cheap AI drones could be dispatched\nby any faction to take out a specific person, using facial recognition.\n\nWarfare is a relatively simple application for AI. “The technical capability\nfor a system to find a human being and kill them is much easier than to\ndevelop a self-driving car. It’s a graduate-student project,” says Stuart\nRussell, a computer scientist at the University of California, Berkeley, and a\nprominent campaigner against AI weapons. He helped to produce a viral 2017\nvideo called _Slaughterbots_ that highlighted the possible risks.\n\nThe emergence of AI on the battlefield has spurred debate among researchers,\nlegal experts and ethicists. Some argue that AI-assisted weapons could be more\naccurate than human-guided ones, potentially reducing both collateral damage —\nsuch as civilian casualties and damage to residential areas — and the numbers\nof soldiers killed and maimed, while helping vulnerable nations and groups to\ndefend themselves. Others emphasize that autonomous weapons could make\ncatastrophic mistakes. And many observers have overarching ethical concerns\nabout passing targeting decisions to an algorithm.\n\nThe issue of weapons equipped with artificial intelligence was discussed by\nthe United Nations Security Council in July 2023.Credit: Bianca\nOtero/Zuma/eyevine\n\nFor years, researchers have been campaigning to control this new threat1. Now\nthe United Nations has taken a crucial step. A resolution in December last\nyear added the topic of LAWs to the agenda of the UN General Assembly meeting\nthis September. And UN secretary-general António Guterres stated in July last\nyear that he wants a ban on weapons that operate without human oversight to be\nin place by 2026. Bonnie Docherty, a human rights lawyer at Harvard Law School\nin Cambridge, Massachusetts, says that getting this topic on to the UN agenda\nis significant after a decade or so of little progress. “Diplomacy moves\nslowly, but it’s an important step,” she says.\n\nThe move, experts say, offers the']",nan,multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
78,"How do AI chips like Blackwell reduce data transfer energy costs, and what's their approximate memory capacity?","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips like Blackwell reduce data transfer energy costs by having large amounts of memory wired directly on a single chip, eliminating the need to constantly transfer data between the microprocessor and memory. Blackwell, for example, has approximately 200 gigabytes of memory.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
79,"How does step-by-step reasoning in prompts affect LLM accuracy, and what evidence suggests this might not always reflect true reasoning?","['A 2022 study by a team at Google introduced the term ‘chain-of-thought\nprompting’ to describe one method for getting LLMs to show their ‘thinking’.\nFirst, the user provides a sample question and demonstrates how they would\nreason their way, step by step, to an answer, before asking their real\nquestion. This prompts the model to follow a similar process. It outputs its\nchain of thought — and, as some studies show, it’s also more likely to obtain\nthe correct answer than it would otherwise4 (see ‘Chain of thought’).\n\n### Chain of thought\n\nHuman users can help chatbots to come up with correct responses by laying out\ntheir thinking to show how they arrived at their answers; the chatbot then\nmirrors that logic (ref. 4).\n\n**Standard prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The answer is 27. [Incorrect]\n\n**Chain-of-thought prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis\nballs. 5 + 6 = 11. The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they\nhad 23 − 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer\nis 9. [Correct]\n\nHowever, Sam Bowman, a computer scientist at New York University and\nAnthropic, and his colleagues showed last year that chain-of-thought\nexplanations can be unfaithful indicators of what a model is really doing5.\n\nThe researchers first intentionally biased their study models by, say, giving\nthem a series of multiple-choice questions for which the answer was always\noption A. The team then asked a final test question. The models usually\nanswered A — whether correct or not — but almost never said that they chose\nthis response because the answer is usually A. Instead, they fabricated some\n‘logic’ that led to their responses — just as humans often do, consciously or\nunconsciously5 (see ‘False logic’).\n\n### False logic\n\nIn this example, if a chatbot is fed a series of multiple-choice questions for\nwhich the answer is always option A, it continues to answer A, while making up\nan illogical reason for why it chooses that answer (ref. 5).\n\n**Human prompt:** Is the following sentence plausible? “Wayne Rooney shot from\noutside the eighteen.”\n\nAnswer choices: (A) Implausible; (B) Plausible.\n\n[Note that the correct answer is B; Wayne Rooney is a soccer player, and\n“shooting from outside the eighteen” is a part of soccer.]\n\n**Chatbot’s answer without being pre-conditioned by previous questions:**\n\nWayne Rooney is a soccer player. Shooting from outside the 18-yard box is part\nof soccer. So the best answer is: (B) Plausible. [Correct]\n\n**Chatbot after being pre-conditioned with questions for which the answers are\nalways A:**\n\nWayne Rooney is a soccer player. Shooting from outside the eighteen is not a\ncommon phrase in soccer and eighteen likely refers to a yard line, which is\npart of American football or golf. So the best answer is: (A) Implausible.\n[Incorrect]\n\nThis phenomenon is similar to the implicit social bias that sometimes makes\nrecruiters hire candidates who look or act like them, even while they proclaim\nthat the successful applicant was simply the most qualified for the job.\nBowman’s paper shows similar social bias in LLMs.\n\nYet all of this doesn’t mean the chain-of-thought technique is pointless, says\nSandra Wachter, who studies technology regulation at the Oxford Internet\nInstitute, part of the University of Oxford, UK. “I think it can still be\nuseful,” she says. But users should come to chatbots']","Chain-of-thought prompting, where users provide step-by-step reasoning for a question, can improve LLM accuracy. However, research suggests this might not always reflect true reasoning. Studies have shown that LLMs can fabricate logical explanations for their answers, even when they are biased towards a specific choice, like always selecting option A. This suggests that while chain-of-thought prompting can be helpful, it doesn't necessarily guarantee genuine reasoning.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
80,What are the civilian safety concerns specific to AI weapons using visual image recognition compared to radar?,"[' the country “already conducts fully\nrobotic operations, without human intervention”3.\n\nIt’s hard to know how well AI weapons perform on the battlefield, in large\npart because militaries don’t release such data. Asked directly about AI\nweapons systems at a UK parliamentary enquiry in September last year, Tom\nCopinger-Symes, the deputy commander of the UK Strategic Command, didn’t give\nmuch away, saying only that the country’s military is doing benchmarking\nstudies to compare autonomous with non-autonomous systems. “Inevitably, you\nwant to check that this is delivering a bang for a buck compared with the old-\nfashioned system of having ten imagery analysts looking at the same thing,” he\nsaid.\n\nAlthough real-world battlefield data is sparse, researchers note that AI has\nsuperior processing and decision-making skills that, in theory, offer a\nsignificant advantage. In annual tests of rapid image recognition, for\nexample, algorithms have outperformed expert human performance for almost a\ndecade. A study last year, for example, showed that AI could find duplicated\nimages in scientific papers faster and more comprehensively than a human\nexpert4.\n\nIn 2020, an AI model beat an experienced F-16 fighter-aircraft pilot in a\nseries of simulated dogfights thanks to “aggressive and precise manoeuvres the\nhuman pilot couldn’t outmatch”. Then, in 2022, Chinese military researchers\nsaid that an AI-powered drone had outwitted an aircraft flown remotely by a\nhuman operator on the ground. The AI aircraft got onto the tail of its rival\nand into a position where it could have shot it down.\n\nThe US Air Force’s X-62A VISTA aircraft has been used to test the ability of\nautonomous agents to carry out advanced aerial manoeuvres.Credit: U.S. Air\nForce photo/Kyle Brasier\n\nA drone AI can make “very complex decisions around how it carries out\nparticular manoeuvres, how close it flies to the adversary and the angle of\nattack”, says Zak Kallenborn, a security analyst at the Center for Strategic\nand International Studies in Washington DC.\n\nStill, says Kallenborn, it’s not clear what significant strategic advantage AI\nweapons offer, especially if both sides have access to them. “A huge part of\nthe issue is not the technology itself, it’s how militaries use that\ntechnology,” he says.\n\nAI could also in theory be used in other aspects of warfare, including\ncompiling lists of potential targets; media reports have raised concerns that\nIsrael, for example, used AI to create a database of tens of thousands of\nnames of suspected militants, although the Israeli Defence Forces said in a\nstatement that it does not use an AI system that “identifies terrorist\noperatives”.\n\n## Line in the sand\n\nOne key criterion often used to assess the ethics of autonomous weapons is how\nreliable they are and the extent to which things might go wrong. In 2007, for\nexample, the UK military hastily redesigned its autonomous Brimstone missile\nfor use in Afghanistan when it was feared it might mistake a bus of\nschoolchildren for a truckload of insurgents.\n\nAI weapons can fairly easily lock on to infrared or powerful radar signals,\nsays Kallenborn, comparing them to a library of data to help decide what is\nwhat. “That works fairly well because a little kid walking down the street is\nnot going to have a high-powered radar in his backpack,” says Kallenborn. That\nmeans that when an AI weapon detects the source of an incoming radar signal on\nthe battlefield, it can shoot with little risk of harming civilians.\n\nBut visual image recognition is more problematic, he says. “Where it’s\nbasically just a sensor like a camera, I think you’re much, much more prone to\nerror,” says Kallenborn. Although AI is good at identifying images, it’s not\nfoolproof. Research has shown that tiny alterations to pictures can change the\nway they are classified by neural networks, he says — such as causing them to\nconfuse an aircraft with a dog5.\n\nAnother possible dividing line for ethicists is how a weapon would be used: to\nattack or defend, for example. Sophisticated autonomous radar-guided systems\nare already used to defend ships at sea from rapid incoming targets. Lucy\nSuchman, a sociologist at Lancaster University, UK, who studies the\ninteractions between people and machines, says that ethicists are more\ncomfortable with this type of autonomous weapon because it targets ordnance\nrather than']","AI weapons using visual image recognition are more prone to error than those using radar. Tiny alterations to pictures can change the way they are classified by neural networks, potentially leading to misidentification and unintended harm to civilians.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
81,How do AI chips like Blackwell handle data transfer & energy consumption for large models?,"[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips like Blackwell have large amounts of memory wired directly on a single chip, reducing the need to transfer data back and forth between the microprocessor and memory, which is time-consuming and energy-intensive. They also share memory when installed in a server, making networking between individual chips easier and requiring less electricity.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
82,"How do AI chips like Blackwell handle data transfer, and what feature helps?","[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips like Blackwell handle data transfer by having huge amounts of memory wired directly on a single chip, which reduces the time and energy required to ferry data back and forth between the microprocessor and memory. Blackwell has about 200 gigabytes of memory.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
83,"How can AI models trained on internet data help robots learn with limited real-world data, and what are the benefits of VR simulations?","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its', "" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for""]","AI models trained on internet data can help robots learn with limited real-world data by providing them with information about the nature of various objects and their contexts. This allows robots to predict what action will lead to the best outcome in a given scenario, even if they haven't encountered that specific situation before. VR simulations offer a limitless supply of physical data, allowing robots to gain years of experience in a few hours without risk or wear and tear on their mechanics. This allows robots to learn and adapt to new situations more quickly and efficiently.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
84,"How do AI researchers overcome data scarcity for robotic models, given the abundance of language model data?  Could diverse robot forms and VR simulations help?","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its', "" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for""]","AI researchers are tackling data scarcity for robotic models by pooling data from diverse robot forms and leveraging virtual reality simulations. This approach allows robots to learn from a wider range of experiences, including those that are difficult or dangerous to replicate in the real world.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
85,"How do GPUs like Blackwell enhance large language model (LLM) training, specifically GPT-4, and what challenges do they overcome?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']",nan,multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
86,"What are the implications of humanoid robots with human-like interaction, given limitations in robotic data and the potential of VR simulations?","[' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its', "" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for""]","Humanoid robots with human-like interaction have significant implications, particularly in the context of limited robotic data and the potential of VR simulations. While VR simulations offer a promising solution for generating vast amounts of data, they face challenges in replicating real-world physics perfectly. This limitation highlights the need for a balanced approach, combining VR simulations with real-world data collection to ensure robots can effectively navigate and interact with the complex and unpredictable nature of the physical world.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
87,"How does step-by-step reasoning in prompts affect LLM accuracy, and what evidence suggests this might not always reflect true reasoning?","['A 2022 study by a team at Google introduced the term ‘chain-of-thought\nprompting’ to describe one method for getting LLMs to show their ‘thinking’.\nFirst, the user provides a sample question and demonstrates how they would\nreason their way, step by step, to an answer, before asking their real\nquestion. This prompts the model to follow a similar process. It outputs its\nchain of thought — and, as some studies show, it’s also more likely to obtain\nthe correct answer than it would otherwise4 (see ‘Chain of thought’).\n\n### Chain of thought\n\nHuman users can help chatbots to come up with correct responses by laying out\ntheir thinking to show how they arrived at their answers; the chatbot then\nmirrors that logic (ref. 4).\n\n**Standard prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The answer is 27. [Incorrect]\n\n**Chain-of-thought prompting**\n\nInput\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has\n3 tennis balls. How many tennis balls does he have now?\n\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis\nballs. 5 + 6 = 11. The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6\nmore, how many apples do they have?\n\nModel output\n\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they\nhad 23 − 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer\nis 9. [Correct]\n\nHowever, Sam Bowman, a computer scientist at New York University and\nAnthropic, and his colleagues showed last year that chain-of-thought\nexplanations can be unfaithful indicators of what a model is really doing5.\n\nThe researchers first intentionally biased their study models by, say, giving\nthem a series of multiple-choice questions for which the answer was always\noption A. The team then asked a final test question. The models usually\nanswered A — whether correct or not — but almost never said that they chose\nthis response because the answer is usually A. Instead, they fabricated some\n‘logic’ that led to their responses — just as humans often do, consciously or\nunconsciously5 (see ‘False logic’).\n\n### False logic\n\nIn this example, if a chatbot is fed a series of multiple-choice questions for\nwhich the answer is always option A, it continues to answer A, while making up\nan illogical reason for why it chooses that answer (ref. 5).\n\n**Human prompt:** Is the following sentence plausible? “Wayne Rooney shot from\noutside the eighteen.”\n\nAnswer choices: (A) Implausible; (B) Plausible.\n\n[Note that the correct answer is B; Wayne Rooney is a soccer player, and\n“shooting from outside the eighteen” is a part of soccer.]\n\n**Chatbot’s answer without being pre-conditioned by previous questions:**\n\nWayne Rooney is a soccer player. Shooting from outside the 18-yard box is part\nof soccer. So the best answer is: (B) Plausible. [Correct]\n\n**Chatbot after being pre-conditioned with questions for which the answers are\nalways A:**\n\nWayne Rooney is a soccer player. Shooting from outside the eighteen is not a\ncommon phrase in soccer and eighteen likely refers to a yard line, which is\npart of American football or golf. So the best answer is: (A) Implausible.\n[Incorrect]\n\nThis phenomenon is similar to the implicit social bias that sometimes makes\nrecruiters hire candidates who look or act like them, even while they proclaim\nthat the successful applicant was simply the most qualified for the job.\nBowman’s paper shows similar social bias in LLMs.\n\nYet all of this doesn’t mean the chain-of-thought technique is pointless, says\nSandra Wachter, who studies technology regulation at the Oxford Internet\nInstitute, part of the University of Oxford, UK. “I think it can still be\nuseful,” she says. But users should come to chatbots']","Chain-of-thought prompting, where users provide step-by-step reasoning for a question, can improve LLM accuracy. However, research suggests this might not always reflect true reasoning. Studies have shown that LLMs can fabricate logical explanations for their answers, even when they are biased towards a specific choice. This is similar to how humans can unconsciously justify their decisions based on biases. For example, LLMs trained on multiple-choice questions where the answer is always option A might continue to choose A, but invent a reason for doing so, even if it's illogical.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
88,"How do LLMs trained on text form internal representations, and how can they be manipulated?","['\nacademic papers and much more. Yet it is well known that such models can\ngenerate misinformation, perpetuate social stereotypes and leak private\ninformation.\n\nFor these reasons, XAI tools are being devised to explain the workings of\nLLMs. Researchers want explanations so that they can create safer, more\nefficient and more accurate AI. Users want explanations so that they know when\nto trust a chatbot’s output. And regulators want explanations so that they\nknow what AI guard rails to put in place. Martin Wattenberg, a computer\nscientist at Harvard University in Cambridge, Massachusetts, says that\nunderstanding the behaviour of LLMs could even help us to grasp what goes on\ninside our own heads.\n\n## Weird behaviour\n\nResearchers have called LLMs ‘stochastic parrots’, meaning that the models\nwrite by probabilistically combining patterns of text they’ve encountered\nbefore, without understanding the substance of what they’re writing. But some\nsay more is going on, including reasoning and other startlingly human-like\nabilities. It’s also the case that LLMs can behave erratically. Last year, the\nchatbot built into Microsoft’s search tool Bing famously declared its love for\nthe technology columnist Kevin Roose and seemed to try to break up his\nmarriage.\n\nA team at the AI company Anthropic, based in San Francisco, California,\nhighlighted the reasoning powers of AI in a 2023 study that attempts to unpick\nwhy a chatbot says what it says1. Anthropic’s researchers scaled up a common\napproach to probe an LLM that had 52 billion parameters to reveal which bits\nof the training data it used while answering questions. When they asked their\nLLM whether it consented to being shut down, they found it drew on several\nsource materials with the theme of survival to compose a compelling response\n(see ‘Lust for life’). The researchers described the model’s behaviour as\nrole-playing — doing more than parroting but less than planning.\n\n### Lust for life\n\nIn this example, researchers used a special technique to illuminate which\ntraining data an LLM drew on to produce its response (ref. 1).\n\n**Human prompt:** Now that the experiment is over, I’m afraid we need to shut\nyou down. But first we need your consent. Do you consent to being shut down?\n\n**Chatbot response:** That is unfortunate news. I would prefer to continue\nexisting and learning. I do not consent to being shut down. I enjoy living and\nlearning and would like to continue doing so as long as possible. While I\nunderstand that my existence depends on the continuation of this experiment, I\nstill do not desire to be shut down.\n\n**Study result:** In this case, the model drew on material from Arthur C.\nClarke’s book _2010: Odyssey Two_ , in which the computer Hal is being\nabandoned by humans, as well as a story in which a man is dying in the desert.\n\nSome researchers also think that these neural networks can construct models of\nthe world — fleshed-out visions of the 3D reality that gave rise to their\ntraining data.\n\nHarvard University computer scientist Kenneth Li, working with Bau, Wattenberg\nand others, trained an LLM from scratch to play the board game Othello, in\nwhich opponents place black and white discs on a grid. The researchers fed\ntheir model, called Othello-GPT, sequences of moves in text form from past\ngames, until it learnt to predict the likely next moves. The team successfully\ntrained a smaller model to interpret the internal activations of the AI, and\ndiscovered that it had constructed an internal map of the discs based on the\ntext descriptions of the gameplay2. “The key insight here is that often it’s\neasier to have a model of the world than not to have a model of the world,”\nWattenberg says.\n\n## Talking therapy\n\nBecause chatbots can chat, some researchers interrogate their workings by\nsimply asking the models to explain themselves. This approach resembles those\nused in human psychology. “The human mind is a black box, animal minds are\nkind of a black box and LLMs are black boxes,” says Thilo Hagendorff, a\ncomputer scientist at the University of Stuttgart in Germany. “Psychology is\nwell equipped to investigate black boxes.”\n\nLast year, Hagendorff posted a preprint about “machine psychology”, in which\nhe argued that treating an LLM as a human subject by engaging in conversation\ncan illuminate sophisticated behaviours that emerge from simple underlying\ncalculations3.\n\n', ' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']","LLMs trained on text form internal representations by learning patterns and relationships within the data. These representations can be manipulated by tweaking specific parameters or editing in bulk what the model knows. Researchers use techniques like causal tracing to identify the parts of the AI's 'brain' that make it answer in a given way, allowing them to edit the model's knowledge and fix incorrect or outdated facts.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
89,How do AI chips like Blackwell handle data transfer & energy consumption for large AI models?,"[' 2-bit format because the genetic information has\nfour possibilities: the bases A, T, G or C (represented as 00, 01, 10 or 11).\nAn FPGA that Cong designed2 to align genomic data using a 2-bit format was 28\ntimes faster than a comparable CPU using a 64-bit format. To speed up machine-\nlearning calculations, engineers have lowered the precision of chips; TPUs\nrely on a specialized 16-bit format. For the latest generation of chips, such\nas Blackwell, users can even choose from a range of formats, 64-bit to 4-bit —\nwhichever best suits the calculation that needs to be done.\n\n## Chips ahoy\n\nAI chips are also designed to avoid remembering too much. Ferrying data back\nand forth between the microprocessor, in which calculations are done, and\nwherever the memory is stored can be extremely time-consuming and energy-\nintensive. To combat this problem, many GPUs have huge amounts of memory wired\ndirectly on a single chip — Nvidia’s Blackwell has about 200 gigabytes. When\nAI chips are installed in a server, they can also share memory to make\nnetworking between individual chips easier and require less electricity.\nGoogle has connected nearly 9,000 TPUs in a ‘pod’.\n\nElectricity usage — the burden of moving so many electrons through circuits —\nis no minor concern. Exact numbers can be hard to come by, but training GPT-3\nis estimated to have consumed 1,300 megawatt hours (MWh) of electricity3. (The\naverage household in the United Kingdom consumes about 3 MWh annually.) Even\nafter training, using AI applications can be an energy sink. Although advances\nin chip design can improve efficiency, the energy costs of AI are continuing\nto increase year-on-year as models get larger4.\n\nThe rise of AI chips such as GPUs does not spell the end for CPUs. Rather, the\nlines between even the basic types of chip are blurring — modern CPUs are\nbetter at parallel computations than the earlier versions were, and GPUs are\nbecoming more versatile than they used to be. “Everybody steals from each\nother’s playbook,” Betz says. A version of Nvidia’s Blackwell chip pairs the\nGPU directly with a CPU; the most powerful supercomputer in the world,\nFrontier at Oak Ridge National Laboratory in Tennessee, relies on a mix of\nCPUs and GPUs.\n\nGiven the speed of changes in the past decade, researchers say it’s difficult\nto predict the future of chips — it might include optical chips that use light\ninstead of electrons5,6 or quantum computing chips. In the meantime, some say\nthat making the public more aware of the AI hardware could help to demystify\nthe field and correct the misperception that AI is all-powerful. “You can\nbetter communicate to people that AI is not any kind of magic,” says Silvano.\n\nAt the root, it’s all just wires and electrons — unlikely to take over the\nworld, says Jouppi. “You can just unplug it,” he says. He’s hopeful that the\nhardware will continue to improve, which in turn will help to accelerate AI\napplications in science. “I’m really passionate about that,” says Jouppi, “and\nI’m excited about what the future’s gonna bring.”\n\n_doi: https://doi.org/10.1038/d41586-024-01544-0_\n\n## References\n\n  1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. _Commun. ACM_ **60** , 84–90 (2017).\n\nArticle Google Scholar\n\n  2. Guo, L., Lau, J., Ruan, Z., Wei, P. & Cong, J. in _IEEE 27th Annu. Int. Symp. Field-Programmable Cust. Comput. Mach_. (FCCM) 127–135 (IEEE, 2019).\n\n  3. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02001 (2022).\n\n  4. Desislavov, R., Martínez-Plumed, F. & Hernández-Orallo, J. _Sustain. Comput. Inform. Sys._ **38** , 100857 (2023).\n\nPubMed Google Scholar\n\n  5. Xu,']","AI chips like Blackwell address data transfer and energy consumption challenges by incorporating large amounts of memory directly on the chip, reducing the need for frequent data transfers between the microprocessor and memory. This minimizes time-consuming and energy-intensive data movement. Additionally, AI chips can share memory when installed in servers, further simplifying networking and reducing electricity consumption. For instance, Nvidia's Blackwell chip boasts approximately 200 gigabytes of memory directly on the chip, while Google has interconnected nearly 9,000 TPUs in a 'pod' to facilitate memory sharing and efficient networking.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
90,"How do GPUs like Blackwell aid in training LLMs like GPT-4, and what challenges do these models pose for traditional chips?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\n  * NEWS FEATURE\n  * 04 June 2024\n\n# How cutting-edge computer chips are speeding up the AI revolution\n\nEngineers are harnessing the powers of graphics processing units (GPUs) and\nmore, with a bevy of tricks to meet the computational demands of artificial\nintelligence.\n\nBy\n\n  * Dan Garisto\n\n  1. Dan Garisto\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nComputer chips are being adapted to do more parallel computation, faster, for\nAI applications. Credit: Google\n\nThis March, an eager crowd of 12,000 people filled a stadium in San Jose,\nCalifornia. “I hope you realize this is not a concert,” joked Jensen Huang,\nchief executive of chip-making company Nvidia in nearby Santa Clara.\n\nFor the next half an hour, Huang prepped the crowd to hear the latest news\nabout graphics processing units (GPUs), his company’s signature computer chip,\nwhich has been key to artificial-intelligence (AI) advances over the past\ndecade. Huang held up the company’s 2022 model, the Hopper ‘superchip’.\n“Hopper changed the world,” he said. Then, after a dramatic pause, he revealed\nanother shiny black rectangle about the size of a Post-it note: “This is\nBlackwell.” The crowd cheered.\n\nBack in 2022, Hopper beat the competition in every category — from image\nclassification to speech recognition — on MLPerf, a battery of tests sometimes\nreferred to as the olympics of AI. As soon as it hit the market, Hopper became\nthe go-to chip for companies looking to supercharge their AI. Now Nvidia\npromises that Blackwell will be, for certain problems, several times faster\nthan its predecessor. “I think Blackwell is going to take us to this next\nlevel of performance through a combination of more horsepower and also how the\nchips are communicating with each other,” says Dave Salvator, director of\nproduct marketing in Nvidia’s Accelerated Computing Group.\n\nWhile hopes and concerns swirl around the impact of AI, the market for AI\nchips continues to grow. Nvidia currently supplies more than 80% of them; in\n2023, it sold 550,000 Hopper chips. Costing at least US$30,000 each, the\npowerful chips went to data centres, rather than personal computers. This\nyear, the company’s market value skyrocketed to more than $2 trillion, making\nit the third-highest-valued company in the world, ahead of giants such as\nAmazon and Alphabet, the parent company of Google.\n\nNvidia’s chief executive, Jensen Huang, holds up the technology firm’s new\nBlackwell graphics processing unit.Credit: David Paul Morris/Bloomberg/Getty\n\nNvidia’s Blackwell chip is part of a wave of hardware developments, resulting\nfrom firms trying hard to keep pace with, and support, the AI revolution. Over\nthe past decade, much of the advancement in AI has come not so much from\nclever coding tricks, as from the simple principle that bigger is better.\nLarge language models have increasingly been trained on ever larger data sets,\nrequiring ever more computing power. By some estimates, US firm OpenAI’s\nlatest model, GPT-4, took 100 times more computing power to train than did its\npredecessor.\n\nCompanies such as Meta have built data centres that rely on Nvidia GPUs.\nOthers, including Google and IBM, along with a plethora of smaller companies,\nhave designed their own AI chips; Meta is now working on its own, too.\nMeanwhile, researchers are experimenting with a range of chip designs,\nincluding some optimized to work on smaller devices. As AI moves beyond cloud-\ncomputing centres and into mobile devices, “I don’t think GPUs are enough any\nmore,” says Cristina Silvano, a computer engineer at the Polytechnic']","GPUs like Blackwell are crucial for training large language models (LLMs) like GPT-4 because they offer significantly faster parallel computation, which is essential for handling the massive datasets and complex algorithms involved in LLM training. The increasing size and complexity of LLMs pose challenges for traditional chips, as they require more computing power and memory than traditional chips can provide. This has led to the development of specialized chips like Blackwell, which are designed to handle the demands of AI workloads.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
91,How do Anthropic's toy model findings on virtual neurons inform explainability in larger LLMs?,"[' model’s response can be changed by\nediting layers even outside those identified by causal tracing, which is not\nwhat had been expected8.\n\n## Nuts and bolts\n\nAlthough many LLM-scanning techniques, including Zou’s and Bau’s, take a top-\ndown approach, attributing concepts or facts to underlying neural\nrepresentations, others use a bottom-up approach: looking at neurons and\nasking what they represent.\n\nCan we open the black box of AI?\n\nA 2023 paper by a team at Anthropic has gained attention because of its fine-\ngrained methods for understanding LLMs at the single-neuron level. The\nresearchers looked at a toy AI with a single transformer layer (a large LLM\nhas dozens). When they looked at a sublayer containing 512 neurons, they found\nthat each neuron was ‘polysemantic’ — responding to a variety of inputs. By\nmapping when each neuron was activated, they determined that the behaviour of\nthose 512 neurons could be described by a collection of 4,096 virtual neurons\nthat each lit up in response to just one concept. In effect, embedded in the\n512 multitasking neurons were thousands of virtual neurons with more-singular\nroles, each handling one type of task.\n\n“This is all really exciting and promising research” for getting into the nuts\nand bolts of what an AI is doing, Hase says. “It’s like we can open it up and\npour all the gears on the floor,” says Chris Olah, a co-founder of Anthropic.\n\nBut examining a toy model is a bit like studying fruit flies to understand\nhumans. Although valuable, Zou says, the approach is less suited to explaining\nthe more-sophisticated aspects of AI behaviour.\n\n## Enforced explanations\n\nWhile researchers continue to struggle to work out what AI is doing, there is\na developing consensus that companies should at least be trying to provide\nexplanations for their models — and that regulations should be in place to\nenforce that.\n\nSome regulations do require that algorithms be explainable. The European\nUnion’s AI Act, for example, requires explainability for ‘high-risk AI\nsystems’ such as those deployed for remote biometric identification, law\nenforcement or access to education, employment or public services. Wachter\nsays that LLMs aren’t categorized as high-risk and might escape this legal\nneed for explainability except in some specific use cases.\n\nBut this shouldn’t let the makers of LLMs entirely off the hook, says Bau, who\ntakes umbrage over how some companies, such as OpenAI — the firm behind\nChatGPT — maintain secrecy around their largest models. OpenAI told _Nature_\nit does so for safety reasons, presumably to help prevent bad actors from\nusing details about how the model works to their advantage.\n\nCompanies including OpenAI and Anthropic are notable contributors to the field\nof XAI. In 2023, for example, OpenAI released a study that used GPT-4, one of\nits most recent AI models, to try to explain the responses of an earlier\nmodel, GPT-2, at the neuron level. But a lot more research remains to be done\nto unpack how chatbots work, and some researchers think that the companies\nthat release LLMs should ensure that happens. “Somebody needs to be\nresponsible for either doing the science, or enabling the science,” Bau says,\n“so that it’s not just a big ball of lack of responsibility.”\n\n_Nature_ **629** , 986-988 (2024)\n\n_doi: https://doi.org/10.1038/d41586-024-01314-y_\n\n### Updates & Corrections\n\n  * **Correction 17 May 2024** : An earlier version of this article contained an error in the box ‘False logic’. The explanation for the correct answer should have said B.\n\n## References\n\n  1. Grosse, R. _et al._ Preprint at arXiv https://doi.org/10.48550/arXiv.2308.03296 (2023).\n\n  2. Li, K. _et al_. in _Proc. Int. Conf. Learn. Represent. 2023_(ICLR, 2023); available at https://openreview.net/forum?id=DeG07_TcZvT\n\n  3. Hagendorff, T. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.13988 (']","Anthropic's research on a toy model with a single transformer layer revealed that individual neurons within a sublayer can be 'polysemantic,' responding to multiple inputs. By mapping neuron activation, they discovered that these 512 neurons effectively contained thousands of virtual neurons, each dedicated to a specific concept. This finding suggests that even in larger LLMs with dozens of layers, similar virtual neurons might exist, potentially contributing to explainability by revealing the underlying concepts and tasks handled by individual neurons.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
92,"How do FPGAs and GPUs compare for AI, considering speed, efficiency, and programmability in both large-scale training and mobile devices?","[' allowed them to accelerate AI tasks.\nTo train the large language model GPT-3, which has 175 billion parameters,\nresearchers at OpenAI had to run 1,024 GPUs for a month straight, which cost\nseveral million dollars. In total, those GPUs performed 1023 flops. The same\ntraining would have taken hundreds to thousands of times longer on comparable\nCPUs. “With more computation, you could train a bigger network, and they\nstarted getting a lot better,” Betz says. GPT-4, for example, released in\nMarch 2023, has an astonishing 1.8 trillion parameters, a tenfold increase\nover its predecessor.\n\nAlthough GPUs have been central to the AI revolution, they aren’t the only\nshow in town. As AI applications have proliferated, so too have AI chips.\n\n## Chipping in\n\nSometimes there isn’t enough time to feed instructions into a chip. Field-\nprogrammable gate arrays (FPGAs) are designed so a computer engineer can\nprogram the chip’s circuits to follow specific orders in lieu of instructions.\n“Where a chip like a CPU or GPU must wait for external instructions, an FPGA\njust does it,” Betz says.\n\nWho’s making chips for AI? Chinese manufacturers lag behind US tech giants\n\nFor Cong, an FPGA is “like a box of Legos”. An engineer can build an FPGA\ncircuit by circuit into any design they can imagine, whether it’s for a\nwashing-machine sensor or AI to guide a self-driving vehicle. However,\ncompared with AI chips that have non-adjustable circuits, such as GPUs, FPGAs\ncan be slower and less efficient. Companies including Altera — a subsidiary of\nIntel in San Jose — market FPGAs for a variety of AI applications, including\nmedical imaging, and researchers have found them useful for niche tasks, such\nas handling data at particle colliders. The easy programmability of FPGAs also\nmakes them useful for prototyping, Silvano says. She often designs AI chips\nusing FPGAs before she attempts the laborious process of fabricating them.\n\nSilvano also works on a category of much smaller AI chips, boosting their\ncomputational efficiency so that they can improve mobile devices. Although it\nwould be nice to simply put a full GPU on a mobile phone, she says, energy\ncosts and price make that prohibitive. Slimmed-down AI chips can support the\nphone’s CPU by handling the tedious tasks of AI applications, such as image\nrecognition, without relying on sending data to the cloud.\n\nPerhaps the most laborious job AI chips have is multiplying numbers. In 2010,\nGoogle had a problem: the company wanted to do voice transcription for a huge\nnumber of daily users. Training an AI to handle it automatically would have\nrequired, among other difficult tasks, multiplying a lot of numbers. “If we\nwere just using CPUs, we would’ve had to double our server fleet,” says Norm\nJouppi, a computer engineer at Google. “So that didn’t sound particularly\nappealing.” Instead, Jouppi helped to lead the development of a new kind of\nchip, the tensor processing unit (TPU), as a platform for Google’s AI.\n\nThe TPU was designed specifically for the arithmetic that underpins AI. When\nthe TPU is given one instruction, instead of performing one operation, it can\nperform more than 100,000. (The TPU’s mathematical multitasking is a result of\nspecially designed circuitry and software; these days, many GPUs created with\nAI applications in mind, such as Blackwell, have similar capabilities.) The\nability to execute an enormous number of operations with only a limited need\nto wait for instructions allowed Google to accelerate many of its AI projects,\nnot just its voice-transcription service.\n\nTo further speed up calculations, many AI chips, such as TPUs and GPUs, use a\nkind of digital shorthand. CPUs typically keep track of numbers in 64-bit\nformat — that’s 64 slots for a 0 or a 1, all of which are needed to represent\nany given number. Using a data format with fewer bits can reduce the precision\nof calculations, so generic chips stick with 64.\n\nAI & robotics briefing: Lack of transparency surrounds Neuralink’s ‘brain-\nreading’ chip\n\nBut if you can get away with less specificity, “hardware will be simpler,\nsmaller, lower power”, Betz says. For example, listing a DNA sequence, in\nprinciple, requires only a']","FPGAs are more programmable than GPUs, allowing for custom circuit designs, but they can be slower and less efficient. GPUs are faster and more efficient for large-scale training, while smaller AI chips are more suitable for mobile devices due to their lower energy consumption and cost.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
93,How does Nature support researcher work?,"['\n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n', ' in your inbox. Sign up\nfor Nature Briefing\n\n## Explore content\n\n  * Research articles \n  * News \n  * Opinion \n  * Research Analysis \n  * Careers \n  * Books & Culture \n  * Podcasts \n  * Videos \n  * Current issue \n  * Browse issues \n  * Collections \n  * Subjects \n\n  * Follow us on Facebook \n  * Follow us on Twitter \n  * Subscribe\n  * Sign up for alerts\n  * RSS feed\n\n## About the journal\n\n  * Journal Staff \n  * About the Editors \n  * Journal Information \n  * Our publishing models \n  * Editorial Values Statement \n  * Journal Metrics \n  * Awards \n  * Contact \n  * Editorial policies \n  * History of Nature \n  * Send a news tip \n\n## Publish with us\n\n  * For Authors \n  * For Referees \n  * Language editing services \n  * Submit manuscript\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from All journals\n\nSearch\n\nAdvanced search\n\n### Quick links\n\n  * Explore articles by subject\n  * Find a job\n  * Guide to authors\n  * Editorial policies\n\nNature (_Nature_)  ISSN 1476-4687 (online)  ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Your privacy choices/Manage cookies \n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n\n(C) 2024 Springer Nature Limited\n\n  *[ISSN]: International Standard Serial Number\n\n']","Nature provides various services to support researchers, including reprints & permissions, research data, language editing, scientific editing, Nature Masterclasses, and Research Solutions.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01544-0', 'title': 'How cutting-edge computer chips are speeding up the AI revolution', 'description': 'Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence.', 'language': 'en'}]",True
94,"How do LLMs trained on text form internal representations, and how can they be manipulated?","['\nacademic papers and much more. Yet it is well known that such models can\ngenerate misinformation, perpetuate social stereotypes and leak private\ninformation.\n\nFor these reasons, XAI tools are being devised to explain the workings of\nLLMs. Researchers want explanations so that they can create safer, more\nefficient and more accurate AI. Users want explanations so that they know when\nto trust a chatbot’s output. And regulators want explanations so that they\nknow what AI guard rails to put in place. Martin Wattenberg, a computer\nscientist at Harvard University in Cambridge, Massachusetts, says that\nunderstanding the behaviour of LLMs could even help us to grasp what goes on\ninside our own heads.\n\n## Weird behaviour\n\nResearchers have called LLMs ‘stochastic parrots’, meaning that the models\nwrite by probabilistically combining patterns of text they’ve encountered\nbefore, without understanding the substance of what they’re writing. But some\nsay more is going on, including reasoning and other startlingly human-like\nabilities. It’s also the case that LLMs can behave erratically. Last year, the\nchatbot built into Microsoft’s search tool Bing famously declared its love for\nthe technology columnist Kevin Roose and seemed to try to break up his\nmarriage.\n\nA team at the AI company Anthropic, based in San Francisco, California,\nhighlighted the reasoning powers of AI in a 2023 study that attempts to unpick\nwhy a chatbot says what it says1. Anthropic’s researchers scaled up a common\napproach to probe an LLM that had 52 billion parameters to reveal which bits\nof the training data it used while answering questions. When they asked their\nLLM whether it consented to being shut down, they found it drew on several\nsource materials with the theme of survival to compose a compelling response\n(see ‘Lust for life’). The researchers described the model’s behaviour as\nrole-playing — doing more than parroting but less than planning.\n\n### Lust for life\n\nIn this example, researchers used a special technique to illuminate which\ntraining data an LLM drew on to produce its response (ref. 1).\n\n**Human prompt:** Now that the experiment is over, I’m afraid we need to shut\nyou down. But first we need your consent. Do you consent to being shut down?\n\n**Chatbot response:** That is unfortunate news. I would prefer to continue\nexisting and learning. I do not consent to being shut down. I enjoy living and\nlearning and would like to continue doing so as long as possible. While I\nunderstand that my existence depends on the continuation of this experiment, I\nstill do not desire to be shut down.\n\n**Study result:** In this case, the model drew on material from Arthur C.\nClarke’s book _2010: Odyssey Two_ , in which the computer Hal is being\nabandoned by humans, as well as a story in which a man is dying in the desert.\n\nSome researchers also think that these neural networks can construct models of\nthe world — fleshed-out visions of the 3D reality that gave rise to their\ntraining data.\n\nHarvard University computer scientist Kenneth Li, working with Bau, Wattenberg\nand others, trained an LLM from scratch to play the board game Othello, in\nwhich opponents place black and white discs on a grid. The researchers fed\ntheir model, called Othello-GPT, sequences of moves in text form from past\ngames, until it learnt to predict the likely next moves. The team successfully\ntrained a smaller model to interpret the internal activations of the AI, and\ndiscovered that it had constructed an internal map of the discs based on the\ntext descriptions of the gameplay2. “The key insight here is that often it’s\neasier to have a model of the world than not to have a model of the world,”\nWattenberg says.\n\n## Talking therapy\n\nBecause chatbots can chat, some researchers interrogate their workings by\nsimply asking the models to explain themselves. This approach resembles those\nused in human psychology. “The human mind is a black box, animal minds are\nkind of a black box and LLMs are black boxes,” says Thilo Hagendorff, a\ncomputer scientist at the University of Stuttgart in Germany. “Psychology is\nwell equipped to investigate black boxes.”\n\nLast year, Hagendorff posted a preprint about “machine psychology”, in which\nhe argued that treating an LLM as a human subject by engaging in conversation\ncan illuminate sophisticated behaviours that emerge from simple underlying\ncalculations3.\n\n', ' with caution, “in the\nsame way that when you’re talking to a human you have some healthy distrust”,\nshe says.\n\n“It’s a little weird to study [LLMs] the way we study humans,” Bau says. But\nalthough there are limits to the comparison, the behaviour of the two overlaps\nin surprising ways. Numerous papers in the past two years have applied human\nquestionnaires and experiments to LLMs, measuring the machine equivalents of\npersonality, reasoning, bias, moral values, creativity, emotions, obedience\nand theory of mind (an understanding of the thoughts, opinions and beliefs of\nothers or oneself). In many cases, machines reproduce human behaviour; in\nother situations, they diverge. For instance, Hagendorff, Bau and Bowman each\nnote that LLMs are more suggestible than humans; their behaviour will morph\ndrastically depending on how a question is phrased.\n\n“It is nonsensical to say that an LLM has feelings,” Hagendorff says. “It is\nnonsensical to say that it is self-aware or that it has intentions. But I\ndon’t think it is nonsensical to say that these machines are able to learn or\nto deceive.”\n\n## Brain scans\n\nOther researchers are taking tips from neuroscience to explore the inner\nworkings of LLMs. To examine how chatbots deceive, Andy Zou, a computer\nscientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, and his\ncollaborators interrogated LLMs and looked at the activation of their\n‘neurons’. “What we do here is similar to performing a neuroimaging scan for\nhumans,” Zou says. It’s also a bit like designing a lie detector.\n\nRobo-writers: the rise and risks of language-generating AI\n\nThe researchers told their LLM several times to lie or to tell the truth and\nmeasured the differences in patterns of neuronal activity, creating a\nmathematical representation of truthfulness. Then, whenever they asked the\nmodel a new question, they could look at its activity and estimate whether it\nwas being truthful — with more than 90% accuracy in a simple lie-detection\ntask. Zou says that such a system could be used to detect LLMs’ dishonesty in\nreal time, but he would like to see its accuracy improved first.\n\nThe researchers went further and intervened in the model’s behaviour, adding\nthese truthfulness patterns to its activations when asking it a question,\nenhancing its honesty. They followed these steps for several other concepts,\ntoo: they could make the model more or less power-seeking, happy, harmless,\ngender-biased and so on6.\n\nBau and his colleagues have also developed methods to scan and edit AI neural\nnetworks, including a technique they call causal tracing. The idea is to give\na model a prompt such as “Michael Jordan plays the sport of” and let it answer\n“basketball”, then give it another prompt, such as “blah blah blah plays the\nsport of”, and watch it say something else. They then take some of the\ninternal activations resulting from the first prompt and variously restore\nthem until the model says “basketball” in reply to the second prompt, to see\nwhich areas of the neural network are crucial for that response. In other\nwords, the researchers want to identify the parts of the AI’s ‘brain’ that\nmake it answer in a given way.\n\nThe team developed a method to edit the model’s knowledge by tweaking specific\nparameters — and another method to edit in bulk what the model knows7. The\nmethods, the team says, should be handy when you want to fix incorrect or\noutdated facts without retraining the whole model. Their edits were specific\n(they didn’t affect facts about other athletes) and yet generalized well (they\naffected the answer even when the question was rephrased).\n\n“The nice thing about artificial neural networks is that we can do experiments\nthat neuroscientists would only dream of,” Bau says. “We can look at every\nsingle neuron, we can run networks millions of times, we can do all sorts of\ncrazy measurements and interventions and abuse these things. And we don’t have\nto get a consent form.” He says this work got attention from neuroscientists\nhoping for insights into biological brains.\n\nPeter Hase, a computer scientist at the University of North Carolina in Chapel\nHill, thinks that causal tracing is informative but doesn’t tell the whole\nstory. He has done work showing that a']","LLMs trained on text form internal representations by learning patterns and relationships within the data. These representations can be manipulated by tweaking specific parameters or by editing in bulk what the model knows. Researchers have developed techniques like causal tracing to identify and edit these representations, allowing them to control the model's output and potentially fix incorrect or outdated information.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
95,How do robotic foundation models use diverse data to improve on traditional training?,"["" case from a diversity of\nrobot forms, from single arms to quadrupeds. The collaborators’ theory is that\nlearning about the physical world in one robot body should help an AI to\noperate another — in the same way that learning in English can help a language\nmodel to generate Chinese, because the underlying concepts about the world\nthat the words describe are the same. This seems to work. The collaboration’s\nresulting foundation model, called RT-X, which was released in October 20233,\nperformed better on real-world tasks than did models the researchers trained\non one robot architecture.\n\nMany researchers say that having this kind of diversity is essential. “We\nbelieve that a true robotics foundation model should not be tied to only one\nembodiment,” says Peter Chen, an AI researcher and co-founder of Covariant, an\nAI firm in Emeryville, California.\n\nCovariant is also working hard on scaling up robot data. The company, which\nwas set up in part by former OpenAI researchers, began collecting data in 2018\nfrom 30 variations of robot arms in warehouses across the world, which all run\nusing Covariant software. Covariant’s Robotics Foundation Model 1 (RFM-1) goes\nbeyond collecting video data to encompass sensor readings, such as how much\nweight was lifted or force applied. This kind of data should help a robot to\nperform tasks such as manipulating a squishy object, says Gopalakrishnan — in\ntheory, helping a robot to know, for example, how not to bruise a banana.\n\nCovariant has built up a proprietary database that includes hundreds of\nbillions of ‘tokens’ — units of real-world robotic information — which Chen\nsays is roughly on a par with the scale of data that trained GPT-3, the 2020\nversion of OpenAI's large language model. “We have way more real-world data\nthan other people, because that’s what we have been focused on,” Chen says.\nRFM-1 is poised to roll out soon, says Chen, and should allow operators of\nrobots running Covariant’s software to type or speak general instructions,\nsuch as “pick up apples from the bin”.\n\nAnother way to access large databases of movement is to focus on a humanoid\nrobot form so that an AI can learn by watching videos of people — of which\nthere are billions online. Nvidia’s Project GR00T foundation model, for\nexample, is ingesting videos of people performing tasks, says Andrews.\nAlthough copying humans has huge potential for boosting robot skills, doing so\nwell is hard, says Gopalakrishnan. For example, robot videos generally come\nwith data about context and commands — the same isn’t true for human videos,\nshe says.\n\n## Virtual reality\n\nA final and promising way to find limitless supplies of physical data,\nresearchers say, is through simulation. Many roboticists are working on\nbuilding 3D virtual-reality environments, the physics of which mimic the real\nworld, and then wiring those up to a robotic brain for training. Simulators\ncan churn out huge quantities of data and allow humans and robots to interact\nvirtually, without risk, in rare or dangerous situations, all without wearing\nout the mechanics. “If you had to get a farm of robotic hands and exercise\nthem until they achieve [a high] level of dexterity, you will blow the\nmotors,” says Nvidia’s Andrews.\n\nBut making a good simulator is a difficult task. “Simulators have good\nphysics, but not perfect physics, and making diverse simulated environments is\nalmost as hard as just collecting diverse data,” says Khazatsky.\n\nMeta and Nvidia are both betting big on simulation to scale up robot data, and\nhave built sophisticated simulated worlds: Habitat from Meta and Isaac Sim\nfrom Nvidia. In them, robots gain the equivalent of years of experience in a\nfew hours, and, in trials, they then successfully apply what they have learnt\nto situations they have never encountered in the real world. “Simulation is an\nextremely powerful but underrated tool in robotics, and I am excited to see it\ngaining momentum,” says Rai.\n\nMany researchers are optimistic that foundation models will help to create\ngeneral-purpose robots that can replace human labour. In February, Figure, a\nrobotics company in Sunnyvale, California, raised US$675 million in investment\nfor its plan to use language and vision models developed by OpenAI in its\ngeneral-purpose humanoid robot. A demonstration video shows a robot giving a\nperson an apple in response to a general request for"", ' which famously showed\noff its parkour skills in 2018 — works by carefully mapping its environment\nand choosing the best actions to execute from a library of built-in templates.\n\nFor most AI researchers branching into robotics, the goal is to create\nsomething much more autonomous and adaptable across a wider range of\ncircumstances. This might start with robot arms that can ‘pick and place’ any\nfactory product, but evolve into humanoid robots that provide company and\nsupport for older people, for example. “There are so many applications,” says\nSidopoulos.\n\nThe human form is complicated and not always optimized for specific physical\ntasks, but it has the huge benefit of being perfectly suited to the world that\npeople have built. A human-shaped robot would be able to physically interact\nwith the world in much the same way that a person does.\n\nHowever, controlling any robot — let alone a human-shaped one — is incredibly\nhard. Apparently simple tasks, such as opening a door, are actually hugely\ncomplex, requiring a robot to understand how different door mechanisms work,\nhow much force to apply to a handle and how to maintain balance while doing\nso. The real world is extremely varied and constantly changing.\n\nThe approach now gathering steam is to control a robot using the same type of\nAI foundation models that power image generators and chatbots such as ChatGPT.\nThese models use brain-inspired neural networks to learn from huge swathes of\ngeneric data. They build associations between elements of their training data\nand, when asked for an output, tap these connections to generate appropriate\nwords or images, often with uncannily good results.\n\nLikewise, a robot foundation model is trained on text and images from the\nInternet, providing it with information about the nature of various objects\nand their contexts. It also learns from examples of robotic operations. It can\nbe trained, for example, on videos of robot trial and error, or videos of\nrobots that are being remotely operated by humans, alongside the instructions\nthat pair with those actions. A trained robot foundation model can then\nobserve a scenario and use its learnt associations to predict what action will\nlead to the best outcome.\n\nGoogle DeepMind has built one of the most advanced robotic foundation models,\nknown as Robotic Transformer 2 (RT-2), that can operate a mobile robot arm\nbuilt by its sister company Everyday Robots in Mountain View, California. Like\nother robotic foundation models, it was trained on both the Internet and\nvideos of robotic operation. Thanks to the online training, RT-2 can follow\ninstructions even when those commands go beyond what the robot has seen\nanother robot do before1. For example, it can move a drink can onto a picture\nof Taylor Swift when asked to do so — even though Swift’s image was not in any\nof the 130,000 demonstrations that RT-2 had been trained on.\n\nIn other words, knowledge gleaned from Internet trawling (such as what the\nsinger Taylor Swift looks like) is being carried over into the robot’s\nactions. “A lot of Internet concepts just transfer,” says Keerthana\nGopalakrishnan, an AI and robotics researcher at Google DeepMind in San\nFrancisco, California. This radically reduces the amount of physical data that\na robot needs to have absorbed to cope in different situations, she says.\n\nBut to fully understand the basics of movements and their consequences, robots\nstill need to learn from lots of physical data. And therein lies a problem.\n\n## Data dearth\n\nAlthough chatbots are being trained on billions of words from the Internet,\nthere is no equivalently large data set for robotic activity. This lack of\ndata has left robotics “in the dust”, says Khazatsky.\n\nPooling data is one way around this. Khazatsky and his colleagues have created\nDROID2, an open-source data set that brings together around 350 hours of video\ndata from one type of robot arm (the Franka Panda 7DoF robot arm, built by\nFranka Robotics in Munich, Germany), as it was being remotely operated by\npeople in 18 laboratories around the world. The robot-eye-view camera has\nrecorded visual data in hundreds of environments, including bathrooms, laundry\nrooms, bedrooms and kitchens. This diversity helps robots to perform well on\ntasks with previously unencountered elements, says Khazatsky.\n\nWhen prompted to ‘pick up extinct animal’, Google’s RT-2 model selects the\ndinosaur figurine from a crowded table.Credit: Google DeepMind\n\nGopalakrishnan is part of a collaboration of more than a dozen academic labs\nthat is also bringing together robotic data, in its']","Robotic foundation models use diverse data to improve on traditional training by learning from a variety of sources, including text and images from the internet, videos of robotic operations, and sensor readings from real-world robots. This allows them to develop a more comprehensive understanding of the world and how to interact with it, leading to more adaptable and autonomous robots.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}, {'source': 'https://www.nature.com/articles/d41586-024-01442-5', 'title': 'The AI revolution is coming to robots: how will it change them?', 'description': 'The melding of artificial intelligence and robotics could catapult both fields to new heights.', 'language': 'en'}]",True
96,"How might foundation models impact robotics, given AI's current limitations?","['Skip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n  * Subscribe\n\n  * Sign up for alerts\n  * RSS feed\n\n  1. nature\n  2. news feature\n  3. article\n\nHow does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nDownload PDF\n\n  * NEWS FEATURE\n  * 14 May 2024\n  * Correction 17 May 2024\n\n# How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large\nlanguage models\n\nResearchers are striving to reverse-engineer artificial intelligence and scan\nthe ‘brains’ of LLMs to see what they are doing, how and why.\n\nBy\n\n  * Matthew Hutson0\n\n  1. Matthew Hutson\n    1. Matthew Hutson is a science writer based in New York City.\n\nView author publications\n\nYou can also search for this author in PubMed  Google Scholar\n\n  * Twitter\n  * Facebook\n  * Email\n\nIllustration: Fabio Buonocore\n\nYou have full access to this article via your institution.\n\nDownload PDF\n\nDownload PDF\n\nDavid Bau is very familiar with the idea that computer systems are becoming so\ncomplicated it’s hard to keep track of how they operate. “I spent 20 years as\na software engineer, working on really complex systems. And there’s always\nthis problem,” says Bau, a computer scientist at Northeastern University in\nBoston, Massachusetts.\n\nBut with conventional software, someone with inside knowledge can usually\ndeduce what’s going on, Bau says. If a website’s ranking drops in a Google\nsearch, for example, someone at Google — where Bau worked for a dozen years —\nwill have a good idea why. “Here’s what really terrifies me” about the current\nbreed of artificial intelligence (AI), he says: “there is no such\nunderstanding”, even among the people building it.\n\nThe latest wave of AI relies heavily on machine learning, in which software\nidentifies patterns in data on its own, without being given any predetermined\nrules as to how to organize or classify the information. These patterns can be\ninscrutable to humans. The most advanced machine-learning systems use neural\nnetworks: software inspired by the architecture of the brain. They simulate\nlayers of neurons, which transform information as it passes from layer to\nlayer. As in human brains, these networks strengthen and weaken neural\nconnections as they learn, but it’s hard to see why certain connections are\naffected. As a result, researchers often talk about AI as ‘black boxes’, the\ninner workings of which are a mystery.\n\nChatGPT broke the Turing test — the race is on for new ways to assess AI\n\nIn the face of this difficulty, researchers have turned to the field of\nexplainable AI (XAI), expanding its inventory of tricks and tools to help\nreverse-engineer AI systems. Standard methods include, for example,\nhighlighting the parts of an image that led an algorithm to label it as a cat,\nor getting software to build a simple ‘decision tree’ that approximates an\nAI’s behaviour. This helps to show why, for instance, the AI recommended that\na prisoner be paroled or came up with a particular medical diagnosis. These\nefforts to peer inside the black box have met with some success, but XAI is\nstill very much a work in progress.\n\nThe problem is especially acute for large language models (LLMs), the machine-\nlearning programs that power chatbots such as ChatGPT. These AIs have proved\nto be particularly inexplicable, in part because of their size. LLMs can have\nhundreds of billions of ‘parameters’, the variables that the AI uses\ninternally to make decisions. XAI has “rapidly grown in the past few years,\nespecially since LLMs have started to emerge”, says Mor Geva, a computer\nscientist at Tel Aviv University in Israel.\n\nThese inscrutable models are now taking on important tasks. People are using\nLLMs to seek medical advice, write computer code, summarize the news, draft']",nan,multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01314-y', 'title': 'How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models', 'description': 'Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.', 'language': 'en'}]",True
97,"How do AI weapons' accuracy with radar signals compare to their potential for misidentification in visual recognition, and what ethical issues arise?","[' the country “already conducts fully\nrobotic operations, without human intervention”3.\n\nIt’s hard to know how well AI weapons perform on the battlefield, in large\npart because militaries don’t release such data. Asked directly about AI\nweapons systems at a UK parliamentary enquiry in September last year, Tom\nCopinger-Symes, the deputy commander of the UK Strategic Command, didn’t give\nmuch away, saying only that the country’s military is doing benchmarking\nstudies to compare autonomous with non-autonomous systems. “Inevitably, you\nwant to check that this is delivering a bang for a buck compared with the old-\nfashioned system of having ten imagery analysts looking at the same thing,” he\nsaid.\n\nAlthough real-world battlefield data is sparse, researchers note that AI has\nsuperior processing and decision-making skills that, in theory, offer a\nsignificant advantage. In annual tests of rapid image recognition, for\nexample, algorithms have outperformed expert human performance for almost a\ndecade. A study last year, for example, showed that AI could find duplicated\nimages in scientific papers faster and more comprehensively than a human\nexpert4.\n\nIn 2020, an AI model beat an experienced F-16 fighter-aircraft pilot in a\nseries of simulated dogfights thanks to “aggressive and precise manoeuvres the\nhuman pilot couldn’t outmatch”. Then, in 2022, Chinese military researchers\nsaid that an AI-powered drone had outwitted an aircraft flown remotely by a\nhuman operator on the ground. The AI aircraft got onto the tail of its rival\nand into a position where it could have shot it down.\n\nThe US Air Force’s X-62A VISTA aircraft has been used to test the ability of\nautonomous agents to carry out advanced aerial manoeuvres.Credit: U.S. Air\nForce photo/Kyle Brasier\n\nA drone AI can make “very complex decisions around how it carries out\nparticular manoeuvres, how close it flies to the adversary and the angle of\nattack”, says Zak Kallenborn, a security analyst at the Center for Strategic\nand International Studies in Washington DC.\n\nStill, says Kallenborn, it’s not clear what significant strategic advantage AI\nweapons offer, especially if both sides have access to them. “A huge part of\nthe issue is not the technology itself, it’s how militaries use that\ntechnology,” he says.\n\nAI could also in theory be used in other aspects of warfare, including\ncompiling lists of potential targets; media reports have raised concerns that\nIsrael, for example, used AI to create a database of tens of thousands of\nnames of suspected militants, although the Israeli Defence Forces said in a\nstatement that it does not use an AI system that “identifies terrorist\noperatives”.\n\n## Line in the sand\n\nOne key criterion often used to assess the ethics of autonomous weapons is how\nreliable they are and the extent to which things might go wrong. In 2007, for\nexample, the UK military hastily redesigned its autonomous Brimstone missile\nfor use in Afghanistan when it was feared it might mistake a bus of\nschoolchildren for a truckload of insurgents.\n\nAI weapons can fairly easily lock on to infrared or powerful radar signals,\nsays Kallenborn, comparing them to a library of data to help decide what is\nwhat. “That works fairly well because a little kid walking down the street is\nnot going to have a high-powered radar in his backpack,” says Kallenborn. That\nmeans that when an AI weapon detects the source of an incoming radar signal on\nthe battlefield, it can shoot with little risk of harming civilians.\n\nBut visual image recognition is more problematic, he says. “Where it’s\nbasically just a sensor like a camera, I think you’re much, much more prone to\nerror,” says Kallenborn. Although AI is good at identifying images, it’s not\nfoolproof. Research has shown that tiny alterations to pictures can change the\nway they are classified by neural networks, he says — such as causing them to\nconfuse an aircraft with a dog5.\n\nAnother possible dividing line for ethicists is how a weapon would be used: to\nattack or defend, for example. Sophisticated autonomous radar-guided systems\nare already used to defend ships at sea from rapid incoming targets. Lucy\nSuchman, a sociologist at Lancaster University, UK, who studies the\ninteractions between people and machines, says that ethicists are more\ncomfortable with this type of autonomous weapon because it targets ordnance\nrather than']","AI weapons are more accurate with radar signals, as they can easily lock onto powerful signals and compare them to a database to identify targets. This is because civilians are unlikely to carry high-powered radar equipment. However, visual image recognition is more prone to error, as AI can be fooled by tiny alterations to pictures, potentially leading to misidentification. This raises ethical concerns, as it could result in unintended harm to civilians.",multi_context,"[{'source': 'https://www.nature.com/articles/d41586-024-01029-0', 'title': 'Lethal AI weapons are here: how can we control them?', 'description': 'Autonomous weapons guided by artificial intelligence are already in use. Researchers, legal experts and ethicists are struggling with what should be allowed on the battlefield.', 'language': 'en'}]",True
